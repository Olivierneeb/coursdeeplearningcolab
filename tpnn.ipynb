{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achanhon/coursdeeplearningcolab/blob/master/tpnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wiLsAyIhStUn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TP réseau de neurones\n",
        "\n",
        "la liste des paquets nécessaires au programme"
      ]
    },
    {
      "metadata": {
        "id": "hGthUy4HS2ju",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.autograd\n",
        "import torch.autograd.variable\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OUQcfvqCS62P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "définition de la fonction objectif : veuillez remarquer que cette fonction est calculable par une réseau de neurones"
      ]
    },
    {
      "metadata": {
        "id": "pI37qd1cVyJD",
        "colab_type": "code",
        "outputId": "2017f8da-1173-458c-aacb-ac40f9c40cd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import animation\n",
        "\n",
        "gridsize = 50\n",
        "\n",
        "class GroundTruth(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GroundTruth, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 4, bias=True)\n",
        "        self.fc2 = nn.Linear(4, 1, bias=True)\n",
        "        self.fc3 = nn.Linear(1, 2, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def forwardnp(self,x):\n",
        "        variablex = torch.autograd.Variable(torch.Tensor(x.astype(float)))\n",
        "        variableoutput = self.forward(variablex)\n",
        "        return variableoutput.cpu().data.numpy()\n",
        "\n",
        "groundtruth = GroundTruth()\n",
        "groundtruth.fc1.weight.data[0][0] = -1\n",
        "groundtruth.fc1.weight.data[0][1] = 0\n",
        "groundtruth.fc1.bias.data[0] = 10\n",
        "groundtruth.fc1.weight.data[1][0] = 1\n",
        "groundtruth.fc1.weight.data[1][1] = 0\n",
        "groundtruth.fc1.bias.data[1] = -30\n",
        "groundtruth.fc1.weight.data[2][0] = 0\n",
        "groundtruth.fc1.weight.data[2][1] = -1\n",
        "groundtruth.fc1.bias.data[2] = 10\n",
        "groundtruth.fc1.weight.data[3][0] = 0\n",
        "groundtruth.fc1.weight.data[3][1] = 1\n",
        "groundtruth.fc1.bias.data[3] = -30\n",
        "\n",
        "groundtruth.fc2.bias.data[0] = 3\n",
        "groundtruth.fc2.weight.data[0][0] = -0.5\n",
        "groundtruth.fc2.weight.data[0][1] = -0.5\n",
        "groundtruth.fc2.weight.data[0][2] = -0.5\n",
        "groundtruth.fc2.weight.data[0][3] = -0.5\n",
        "\n",
        "groundtruth.fc3.bias.data[0] = -0.5\n",
        "groundtruth.fc3.weight.data[0][0] = 1\n",
        "groundtruth.fc3.weight.data[1] = -groundtruth.fc3.weight.data[0]\n",
        "groundtruth.fc3.bias.data[1] = -groundtruth.fc3.bias.data[0]\n",
        "groundtruth.eval()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GroundTruth(\n",
              "  (fc1): Linear(in_features=2, out_features=4, bias=True)\n",
              "  (fc2): Linear(in_features=4, out_features=1, bias=True)\n",
              "  (fc3): Linear(in_features=1, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "gp88MHnQTXbK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "malheureusement de cette fonction on ne connait qu'un certain nombre d'échantillons"
      ]
    },
    {
      "metadata": {
        "id": "Uj_cta7CTWw-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "samples = np.random.randint(0,50, size=(50,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vrTyBA5vTXAl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "et, à partir de ces échantillons seulement, on veut estimer la fonction (ce qui est impossible dans l'absolue mais qui peut marcher en pratique vue que les fonctions qu'on cherche à estimer son *régulière*).\n",
        "Pour cela on définit un réseau :"
      ]
    },
    {
      "metadata": {
        "id": "tIAVxBupTSJk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "34de954b-9cbb-41ea-8e7c-cd690809dbf0"
      },
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 30, bias=True)\n",
        "        self.fc2 = nn.Linear(30, 30, bias=True)\n",
        "        self.fc2bis = nn.Linear(30, 30, bias=True)\n",
        "        self.fc3 = nn.Linear(30, 2, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x/30))\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = F.leaky_relu(self.fc2bis(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def forwardnp(self,x):\n",
        "        variablex = torch.autograd.Variable(torch.Tensor(x.astype(float)))\n",
        "        variableoutput = self.forward(variablex)\n",
        "        return variableoutput.cpu().data.numpy()\n",
        "\n",
        "model = Net()\n",
        "model.train()\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (fc1): Linear(in_features=2, out_features=30, bias=True)\n",
              "  (fc2): Linear(in_features=30, out_features=30, bias=True)\n",
              "  (fc2bis): Linear(in_features=30, out_features=30, bias=True)\n",
              "  (fc3): Linear(in_features=30, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "_tAZm4eeTRl0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "et on va apprendre sur les échantillons\n",
        "\n",
        "ici juste quelques fonctions d'affichage "
      ]
    },
    {
      "metadata": {
        "id": "bN55wILyUCHL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def visualizemodel(model,visufond,visusample):\n",
        "    grid = np.zeros((gridsize,gridsize,3),dtype=int)\n",
        "    grid[:]=255\n",
        "\n",
        "    batch = np.zeros((gridsize*gridsize,2),dtype=int)\n",
        "    for row in range(gridsize):\n",
        "        for col in range(gridsize):\n",
        "            batch[row*gridsize+col][0]=row\n",
        "            batch[row*gridsize+col][1]=col\n",
        "\n",
        "    prob = model.forwardnp(batch)\n",
        "    pred = np.argmax(prob,axis=1)\n",
        "\n",
        "    if visufond:\n",
        "        grid[:,:,0]=175\n",
        "        for row in range(gridsize):\n",
        "            for col in range(gridsize):\n",
        "                if pred[row*gridsize+col]==1:\n",
        "                    grid[row][col][1] = 175\n",
        "                else:\n",
        "                    grid[row][col][2] = 175\n",
        "\n",
        "    if visusample:\n",
        "        for row,col in samples:\n",
        "            grid[row][col][0]=0\n",
        "            if pred[row*gridsize+col]==1:\n",
        "                grid[row][col][1] = 0\n",
        "            else:\n",
        "                grid[row][col][2] = 0\n",
        "\n",
        "    return np.uint8(grid),pred\n",
        "\n",
        "def visualizeALL():\n",
        "    gt,gtpred = visualizemodel(groundtruth,True,False)\n",
        "    justsamples,_ = visualizemodel(groundtruth,False,True)\n",
        "    model.eval()\n",
        "    modelout,pred = visualizemodel(model,True,True)\n",
        "    model.train()\n",
        "    \n",
        "    samplemask = np.zeros(gridsize*gridsize,dtype=int)\n",
        "    for row,col in samples:\n",
        "        samplemask[row*gridsize+col] = 1\n",
        "    realerror = np.sum(np.absolute(gtpred-pred))\n",
        "    observederror = np.sum(np.absolute(gtpred-pred)*samplemask)\n",
        "    \n",
        "    return np.concatenate((gt,justsamples,modelout), axis=1),realerror,observederror\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "enUj8a0rUW2g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "les paramètres de l'apprentissage"
      ]
    },
    {
      "metadata": {
        "id": "kIxR_avk8hVM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lr = 0.1\n",
        "momentum = 0.5\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "losslayer = nn.CrossEntropyLoss()\n",
        "nbepoch = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rml7pp2SUbr2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "l'apprentissage"
      ]
    },
    {
      "metadata": {
        "id": "CRvyY5yRRJGw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3672
        },
        "outputId": "d51d96f2-ab16-40fd-f26d-bfd09f8a15b3"
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output # command to clear the figures\n",
        "from time import sleep\n",
        "\n",
        "allprints = []\n",
        "for epoch in range(nbepoch):\n",
        "    batch = torch.autograd.Variable(torch.Tensor(samples.astype(float)).float())\n",
        "    target = torch.autograd.Variable(torch.from_numpy(np.argmax(groundtruth.forwardnp(samples),axis=1)).long())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(batch)\n",
        "\n",
        "    loss = losslayer(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "    grid,realerror,observederror = visualizeALL()\n",
        "    allprints.append((\"real error=\",realerror,\"\\tobserved error on samples=\", observederror, \"\\toptimisation loss=\", loss.cpu().data.numpy()))\n",
        "    \n",
        "    if epoch%8==0:\n",
        "        #show how it learn\n",
        "        clear_output()\n",
        "        plt.imshow(grid)\n",
        "        plt.show()\n",
        "        sleep(1)\n",
        "\n",
        "#print all the log at the end of the loop (otherwise you do not see the plot)\n",
        "for a,b,c,d,e,f in allprints:\n",
        "    print(a,b,c,d,e,f)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAC2CAYAAACYjId+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGEbzBQDAMJovAACG/Q9ZKwThyzvZmQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.5872671\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.58501065\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.5826737\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.5802513\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.577759\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.5751996\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.5725545\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.5698205\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.5669856\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.5640572\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.56101984\n",
            "real error= 928 \tobserved error on samples= 17 \toptimisation loss= 0.5578815\n",
            "real error= 954 \tobserved error on samples= 17 \toptimisation loss= 0.55465835\n",
            "real error= 985 \tobserved error on samples= 17 \toptimisation loss= 0.5513342\n",
            "real error= 1006 \tobserved error on samples= 17 \toptimisation loss= 0.54789084\n",
            "real error= 1006 \tobserved error on samples= 16 \toptimisation loss= 0.54434836\n",
            "real error= 1001 \tobserved error on samples= 16 \toptimisation loss= 0.54072523\n",
            "real error= 991 \tobserved error on samples= 17 \toptimisation loss= 0.5369928\n",
            "real error= 982 \tobserved error on samples= 17 \toptimisation loss= 0.5331475\n",
            "real error= 970 \tobserved error on samples= 16 \toptimisation loss= 0.5291981\n",
            "real error= 957 \tobserved error on samples= 14 \toptimisation loss= 0.5251537\n",
            "real error= 943 \tobserved error on samples= 14 \toptimisation loss= 0.52102137\n",
            "real error= 932 \tobserved error on samples= 13 \toptimisation loss= 0.5168151\n",
            "real error= 919 \tobserved error on samples= 13 \toptimisation loss= 0.51252836\n",
            "real error= 907 \tobserved error on samples= 13 \toptimisation loss= 0.5081432\n",
            "real error= 889 \tobserved error on samples= 13 \toptimisation loss= 0.5036571\n",
            "real error= 875 \tobserved error on samples= 14 \toptimisation loss= 0.49898133\n",
            "real error= 860 \tobserved error on samples= 14 \toptimisation loss= 0.4941676\n",
            "real error= 845 \tobserved error on samples= 12 \toptimisation loss= 0.48923886\n",
            "real error= 832 \tobserved error on samples= 12 \toptimisation loss= 0.48423472\n",
            "real error= 818 \tobserved error on samples= 11 \toptimisation loss= 0.4791668\n",
            "real error= 803 \tobserved error on samples= 11 \toptimisation loss= 0.4739771\n",
            "real error= 789 \tobserved error on samples= 11 \toptimisation loss= 0.46867982\n",
            "real error= 781 \tobserved error on samples= 11 \toptimisation loss= 0.46328267\n",
            "real error= 767 \tobserved error on samples= 10 \toptimisation loss= 0.45788985\n",
            "real error= 755 \tobserved error on samples= 11 \toptimisation loss= 0.45239696\n",
            "real error= 744 \tobserved error on samples= 11 \toptimisation loss= 0.44682285\n",
            "real error= 739 \tobserved error on samples= 10 \toptimisation loss= 0.4411327\n",
            "real error= 718 \tobserved error on samples= 10 \toptimisation loss= 0.4352781\n",
            "real error= 709 \tobserved error on samples= 10 \toptimisation loss= 0.4292548\n",
            "real error= 700 \tobserved error on samples= 10 \toptimisation loss= 0.4230541\n",
            "real error= 690 \tobserved error on samples= 10 \toptimisation loss= 0.41675368\n",
            "real error= 677 \tobserved error on samples= 10 \toptimisation loss= 0.41032875\n",
            "real error= 667 \tobserved error on samples= 9 \toptimisation loss= 0.40384397\n",
            "real error= 657 \tobserved error on samples= 9 \toptimisation loss= 0.3972163\n",
            "real error= 645 \tobserved error on samples= 9 \toptimisation loss= 0.39056775\n",
            "real error= 634 \tobserved error on samples= 9 \toptimisation loss= 0.38385165\n",
            "real error= 624 \tobserved error on samples= 9 \toptimisation loss= 0.37701416\n",
            "real error= 615 \tobserved error on samples= 9 \toptimisation loss= 0.37013215\n",
            "real error= 611 \tobserved error on samples= 9 \toptimisation loss= 0.36319473\n",
            "real error= 595 \tobserved error on samples= 9 \toptimisation loss= 0.35617888\n",
            "real error= 582 \tobserved error on samples= 9 \toptimisation loss= 0.3490517\n",
            "real error= 575 \tobserved error on samples= 9 \toptimisation loss= 0.34201545\n",
            "real error= 574 \tobserved error on samples= 9 \toptimisation loss= 0.33512372\n",
            "real error= 569 \tobserved error on samples= 9 \toptimisation loss= 0.32826722\n",
            "real error= 558 \tobserved error on samples= 9 \toptimisation loss= 0.32146278\n",
            "real error= 551 \tobserved error on samples= 8 \toptimisation loss= 0.314826\n",
            "real error= 547 \tobserved error on samples= 8 \toptimisation loss= 0.30839378\n",
            "real error= 543 \tobserved error on samples= 8 \toptimisation loss= 0.30209908\n",
            "real error= 531 \tobserved error on samples= 7 \toptimisation loss= 0.29585797\n",
            "real error= 524 \tobserved error on samples= 7 \toptimisation loss= 0.2897079\n",
            "real error= 519 \tobserved error on samples= 7 \toptimisation loss= 0.28370503\n",
            "real error= 513 \tobserved error on samples= 7 \toptimisation loss= 0.27784085\n",
            "real error= 502 \tobserved error on samples= 7 \toptimisation loss= 0.27205995\n",
            "real error= 500 \tobserved error on samples= 6 \toptimisation loss= 0.26644444\n",
            "real error= 496 \tobserved error on samples= 6 \toptimisation loss= 0.26101306\n",
            "real error= 495 \tobserved error on samples= 6 \toptimisation loss= 0.2557217\n",
            "real error= 489 \tobserved error on samples= 6 \toptimisation loss= 0.25033155\n",
            "real error= 487 \tobserved error on samples= 6 \toptimisation loss= 0.24514598\n",
            "real error= 487 \tobserved error on samples= 5 \toptimisation loss= 0.24001713\n",
            "real error= 485 \tobserved error on samples= 4 \toptimisation loss= 0.23509654\n",
            "real error= 479 \tobserved error on samples= 4 \toptimisation loss= 0.23038018\n",
            "real error= 478 \tobserved error on samples= 4 \toptimisation loss= 0.2258796\n",
            "real error= 477 \tobserved error on samples= 4 \toptimisation loss= 0.2215404\n",
            "real error= 474 \tobserved error on samples= 4 \toptimisation loss= 0.21740925\n",
            "real error= 475 \tobserved error on samples= 4 \toptimisation loss= 0.21344626\n",
            "real error= 470 \tobserved error on samples= 3 \toptimisation loss= 0.20963871\n",
            "real error= 467 \tobserved error on samples= 3 \toptimisation loss= 0.20598505\n",
            "real error= 466 \tobserved error on samples= 3 \toptimisation loss= 0.20247754\n",
            "real error= 468 \tobserved error on samples= 3 \toptimisation loss= 0.19909382\n",
            "real error= 466 \tobserved error on samples= 3 \toptimisation loss= 0.19582736\n",
            "real error= 466 \tobserved error on samples= 3 \toptimisation loss= 0.19265248\n",
            "real error= 469 \tobserved error on samples= 3 \toptimisation loss= 0.1895786\n",
            "real error= 465 \tobserved error on samples= 3 \toptimisation loss= 0.18662795\n",
            "real error= 457 \tobserved error on samples= 3 \toptimisation loss= 0.18374598\n",
            "real error= 457 \tobserved error on samples= 3 \toptimisation loss= 0.18091877\n",
            "real error= 458 \tobserved error on samples= 3 \toptimisation loss= 0.1781442\n",
            "real error= 458 \tobserved error on samples= 3 \toptimisation loss= 0.17541979\n",
            "real error= 459 \tobserved error on samples= 3 \toptimisation loss= 0.1728355\n",
            "real error= 458 \tobserved error on samples= 3 \toptimisation loss= 0.17031881\n",
            "real error= 457 \tobserved error on samples= 3 \toptimisation loss= 0.16788131\n",
            "real error= 455 \tobserved error on samples= 3 \toptimisation loss= 0.16549028\n",
            "real error= 458 \tobserved error on samples= 3 \toptimisation loss= 0.16311824\n",
            "real error= 457 \tobserved error on samples= 3 \toptimisation loss= 0.16077234\n",
            "real error= 455 \tobserved error on samples= 3 \toptimisation loss= 0.15849054\n",
            "real error= 454 \tobserved error on samples= 3 \toptimisation loss= 0.15622444\n",
            "real error= 454 \tobserved error on samples= 3 \toptimisation loss= 0.15398888\n",
            "real error= 453 \tobserved error on samples= 3 \toptimisation loss= 0.15179965\n",
            "real error= 451 \tobserved error on samples= 3 \toptimisation loss= 0.14965643\n",
            "real error= 449 \tobserved error on samples= 3 \toptimisation loss= 0.14755976\n",
            "real error= 449 \tobserved error on samples= 3 \toptimisation loss= 0.1454453\n",
            "real error= 446 \tobserved error on samples= 2 \toptimisation loss= 0.14339496\n",
            "real error= 450 \tobserved error on samples= 2 \toptimisation loss= 0.14123036\n",
            "real error= 443 \tobserved error on samples= 2 \toptimisation loss= 0.13911699\n",
            "real error= 442 \tobserved error on samples= 2 \toptimisation loss= 0.13697201\n",
            "real error= 441 \tobserved error on samples= 2 \toptimisation loss= 0.13490361\n",
            "real error= 438 \tobserved error on samples= 2 \toptimisation loss= 0.13277562\n",
            "real error= 437 \tobserved error on samples= 2 \toptimisation loss= 0.13070078\n",
            "real error= 434 \tobserved error on samples= 2 \toptimisation loss= 0.12860076\n",
            "real error= 434 \tobserved error on samples= 2 \toptimisation loss= 0.12666047\n",
            "real error= 433 \tobserved error on samples= 2 \toptimisation loss= 0.12473772\n",
            "real error= 430 \tobserved error on samples= 2 \toptimisation loss= 0.122878335\n",
            "real error= 431 \tobserved error on samples= 2 \toptimisation loss= 0.12099109\n",
            "real error= 428 \tobserved error on samples= 2 \toptimisation loss= 0.119173184\n",
            "real error= 425 \tobserved error on samples= 2 \toptimisation loss= 0.117314756\n",
            "real error= 423 \tobserved error on samples= 2 \toptimisation loss= 0.11547862\n",
            "real error= 419 \tobserved error on samples= 2 \toptimisation loss= 0.11380745\n",
            "real error= 421 \tobserved error on samples= 2 \toptimisation loss= 0.11204986\n",
            "real error= 421 \tobserved error on samples= 2 \toptimisation loss= 0.110355355\n",
            "real error= 416 \tobserved error on samples= 2 \toptimisation loss= 0.10862868\n",
            "real error= 417 \tobserved error on samples= 2 \toptimisation loss= 0.10701953\n",
            "real error= 414 \tobserved error on samples= 2 \toptimisation loss= 0.10529074\n",
            "real error= 412 \tobserved error on samples= 2 \toptimisation loss= 0.10377222\n",
            "real error= 414 \tobserved error on samples= 2 \toptimisation loss= 0.10216645\n",
            "real error= 413 \tobserved error on samples= 2 \toptimisation loss= 0.100515425\n",
            "real error= 410 \tobserved error on samples= 2 \toptimisation loss= 0.099003404\n",
            "real error= 408 \tobserved error on samples= 2 \toptimisation loss= 0.0975198\n",
            "real error= 408 \tobserved error on samples= 2 \toptimisation loss= 0.09599573\n",
            "real error= 408 \tobserved error on samples= 2 \toptimisation loss= 0.09453733\n",
            "real error= 406 \tobserved error on samples= 2 \toptimisation loss= 0.09333052\n",
            "real error= 403 \tobserved error on samples= 1 \toptimisation loss= 0.09175955\n",
            "real error= 403 \tobserved error on samples= 1 \toptimisation loss= 0.09036111\n",
            "real error= 399 \tobserved error on samples= 1 \toptimisation loss= 0.08908664\n",
            "real error= 397 \tobserved error on samples= 1 \toptimisation loss= 0.087877944\n",
            "real error= 393 \tobserved error on samples= 1 \toptimisation loss= 0.08669077\n",
            "real error= 395 \tobserved error on samples= 1 \toptimisation loss= 0.08528117\n",
            "real error= 392 \tobserved error on samples= 1 \toptimisation loss= 0.08401176\n",
            "real error= 393 \tobserved error on samples= 1 \toptimisation loss= 0.082785465\n",
            "real error= 392 \tobserved error on samples= 1 \toptimisation loss= 0.081534036\n",
            "real error= 391 \tobserved error on samples= 1 \toptimisation loss= 0.080415994\n",
            "real error= 389 \tobserved error on samples= 1 \toptimisation loss= 0.07927814\n",
            "real error= 392 \tobserved error on samples= 1 \toptimisation loss= 0.078213185\n",
            "real error= 389 \tobserved error on samples= 1 \toptimisation loss= 0.077112556\n",
            "real error= 388 \tobserved error on samples= 1 \toptimisation loss= 0.07559981\n",
            "real error= 387 \tobserved error on samples= 1 \toptimisation loss= 0.07466836\n",
            "real error= 387 \tobserved error on samples= 1 \toptimisation loss= 0.0733526\n",
            "real error= 384 \tobserved error on samples= 1 \toptimisation loss= 0.072173625\n",
            "real error= 380 \tobserved error on samples= 1 \toptimisation loss= 0.0711437\n",
            "real error= 378 \tobserved error on samples= 1 \toptimisation loss= 0.07074194\n",
            "real error= 376 \tobserved error on samples= 1 \toptimisation loss= 0.06933881\n",
            "real error= 379 \tobserved error on samples= 1 \toptimisation loss= 0.06836093\n",
            "real error= 375 \tobserved error on samples= 1 \toptimisation loss= 0.067390606\n",
            "real error= 375 \tobserved error on samples= 1 \toptimisation loss= 0.06646327\n",
            "real error= 371 \tobserved error on samples= 1 \toptimisation loss= 0.065152384\n",
            "real error= 372 \tobserved error on samples= 1 \toptimisation loss= 0.06412248\n",
            "real error= 374 \tobserved error on samples= 1 \toptimisation loss= 0.06328802\n",
            "real error= 370 \tobserved error on samples= 1 \toptimisation loss= 0.062458154\n",
            "real error= 372 \tobserved error on samples= 1 \toptimisation loss= 0.061750706\n",
            "real error= 367 \tobserved error on samples= 1 \toptimisation loss= 0.060992014\n",
            "real error= 371 \tobserved error on samples= 1 \toptimisation loss= 0.060659353\n",
            "real error= 368 \tobserved error on samples= 1 \toptimisation loss= 0.05968543\n",
            "real error= 369 \tobserved error on samples= 1 \toptimisation loss= 0.0588449\n",
            "real error= 364 \tobserved error on samples= 1 \toptimisation loss= 0.05786279\n",
            "real error= 369 \tobserved error on samples= 1 \toptimisation loss= 0.057127498\n",
            "real error= 361 \tobserved error on samples= 1 \toptimisation loss= 0.05602377\n",
            "real error= 364 \tobserved error on samples= 1 \toptimisation loss= 0.05519887\n",
            "real error= 360 \tobserved error on samples= 1 \toptimisation loss= 0.054261494\n",
            "real error= 365 \tobserved error on samples= 0 \toptimisation loss= 0.053970903\n",
            "real error= 354 \tobserved error on samples= 1 \toptimisation loss= 0.05348218\n",
            "real error= 357 \tobserved error on samples= 0 \toptimisation loss= 0.053231526\n",
            "real error= 356 \tobserved error on samples= 1 \toptimisation loss= 0.05211415\n",
            "real error= 360 \tobserved error on samples= 0 \toptimisation loss= 0.051599815\n",
            "real error= 352 \tobserved error on samples= 1 \toptimisation loss= 0.05092924\n",
            "real error= 358 \tobserved error on samples= 0 \toptimisation loss= 0.050160658\n",
            "real error= 353 \tobserved error on samples= 1 \toptimisation loss= 0.049468298\n",
            "real error= 356 \tobserved error on samples= 1 \toptimisation loss= 0.04836546\n",
            "real error= 353 \tobserved error on samples= 1 \toptimisation loss= 0.047775075\n",
            "real error= 351 \tobserved error on samples= 1 \toptimisation loss= 0.046969425\n",
            "real error= 353 \tobserved error on samples= 0 \toptimisation loss= 0.04665431\n",
            "real error= 350 \tobserved error on samples= 1 \toptimisation loss= 0.045979377\n",
            "real error= 355 \tobserved error on samples= 0 \toptimisation loss= 0.045602113\n",
            "real error= 346 \tobserved error on samples= 1 \toptimisation loss= 0.045598127\n",
            "real error= 360 \tobserved error on samples= 0 \toptimisation loss= 0.045278504\n",
            "real error= 349 \tobserved error on samples= 1 \toptimisation loss= 0.04578064\n",
            "real error= 354 \tobserved error on samples= 0 \toptimisation loss= 0.04482249\n",
            "real error= 344 \tobserved error on samples= 1 \toptimisation loss= 0.043536354\n",
            "real error= 347 \tobserved error on samples= 0 \toptimisation loss= 0.04315531\n",
            "real error= 346 \tobserved error on samples= 0 \toptimisation loss= 0.042095777\n",
            "real error= 343 \tobserved error on samples= 1 \toptimisation loss= 0.041283198\n",
            "real error= 351 \tobserved error on samples= 0 \toptimisation loss= 0.04085166\n",
            "real error= 342 \tobserved error on samples= 1 \toptimisation loss= 0.040973097\n",
            "real error= 350 \tobserved error on samples= 0 \toptimisation loss= 0.04084386\n",
            "real error= 343 \tobserved error on samples= 1 \toptimisation loss= 0.03991651\n",
            "real error= 347 \tobserved error on samples= 0 \toptimisation loss= 0.03932127\n",
            "real error= 340 \tobserved error on samples= 1 \toptimisation loss= 0.038831953\n",
            "real error= 343 \tobserved error on samples= 0 \toptimisation loss= 0.038532242\n",
            "real error= 337 \tobserved error on samples= 0 \toptimisation loss= 0.037871167\n",
            "real error= 346 \tobserved error on samples= 0 \toptimisation loss= 0.03708992\n",
            "real error= 334 \tobserved error on samples= 0 \toptimisation loss= 0.03736773\n",
            "real error= 339 \tobserved error on samples= 0 \toptimisation loss= 0.036670014\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}