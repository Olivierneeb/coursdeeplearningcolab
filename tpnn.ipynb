{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achanhon/coursdeeplearningcolab/blob/master/tpnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiLsAyIhStUn",
        "colab_type": "text"
      },
      "source": [
        "# TP réseau de neurones\n",
        "\n",
        "### Objectif\n",
        "L'objectif de ce tp est de se familiariser avec la paradigme essentielle de l'apprentissage par ordinateur : une base d'apprentissage pour apprendre et une base de test pour évaluer.\n",
        "\n",
        "### Demarche\n",
        "La demarche repose principalement sur la recherche de jeux de données apprentissage et test sur lesquels un algorithme fixé a un comportement demandé.\n",
        "\n",
        "Indépendamment, l'ensemble du code étant modifiable, il invite à être modifié (l'algorithme principal peut notamment être remplacé par un plus proche voisin) pour explorer différemment ces notions. De plus la selection de la répartition train/test peut être faites aléatoirement à partir d'une distribution fixe.\n",
        "\n",
        "### À noter\n",
        "À noter qu'une version beaucoup plus \"cool\" peut être trouvé ici : https://playground.tensorflow.org avec le désavantage de mettre l'accent sur le réseau et non sur les données comme c'est l'objectif ici.\n",
        "\n",
        "Néanmoins, compte tenu de la qualité de cette application (n'est pas google qui veut), le tp peut +/- être fait sur ce site plutôt que dans ce notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1SZHHMEIDZZ",
        "colab_type": "text"
      },
      "source": [
        "## La boite noire\n",
        "\n",
        "L'ensemble des fonctions ci dessous n'est pas voué à être comprise/modifié en première lecture.\n",
        "Cela implémente une fonction qui étant donnée une base d'apprentissage et une base de test, réalise une apprentissage et une évaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGthUy4HS2ju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.autograd\n",
        "import torch.autograd.variable\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import animation\n",
        "\n",
        "def visualize_current_model_behaviour(trainingdata,testingdata,model):\n",
        "    data = {}\n",
        "    data[\"train\"] = trainingdata\n",
        "    data[\"test\"] = testingdata\n",
        "    \n",
        "    grid = {}\n",
        "    grid[\"train\"] = np.ones((50,50,3),dtype=int)\n",
        "    grid[\"test\"] = np.ones((50,50,3),dtype=int)\n",
        "    for traintest in [\"train\",\"test\"]:\n",
        "        X,Y = data[traintest]\n",
        "        for i in range(X.shape[0]):\n",
        "            row,col = X[i][0],X[i][1]\n",
        "            if Y[i]==1:\n",
        "                grid[traintest][row][col][0]=0\n",
        "                grid[traintest][row][col][2]=0\n",
        "            else:\n",
        "                grid[traintest][row][col][0]=0\n",
        "                grid[traintest][row][col][1]=0\n",
        "    \n",
        "    grid[\"alldata\"] = grid[\"train\"]*grid[\"test\"]\n",
        "    \n",
        "    batch = np.zeros((50*50,2),dtype=float)\n",
        "    for row in range(50):\n",
        "        for col in range(50):\n",
        "            batch[row*50+col][0]=row\n",
        "            batch[row*50+col][1]=col\n",
        "    prediction = model.getPredictedClass(batch)\n",
        "    \n",
        "    \n",
        "    grid[\"pred\"] = np.ones((50,50,3),dtype=int)*255\n",
        "    for row in range(50):\n",
        "        for col in range(50):\n",
        "            if prediction[row*50+col] == 1:\n",
        "                grid[\"pred\"][row][col][0]=175\n",
        "                grid[\"pred\"][row][col][2]=175\n",
        "            else:\n",
        "                grid[\"pred\"][row][col][0]=175\n",
        "                grid[\"pred\"][row][col][1]=175\n",
        "    \n",
        "    grid[\"train_error\"] = np.ones((50,50,3),dtype=int)*255\n",
        "    grid[\"test_error\"] = np.ones((50,50,3),dtype=int)*255\n",
        "    for traintest in [\"train\",\"test\"]:\n",
        "        X,Y = data[traintest]\n",
        "        for i in range(X.shape[0]):\n",
        "            row,col = X[i][0],X[i][1]\n",
        "            if prediction[row*50+col] != Y[i]:\n",
        "                grid[traintest+\"_error\"][row][col][1]=0\n",
        "                grid[traintest+\"_error\"][row][col][2]=0\n",
        "    \n",
        "    for key in grid:\n",
        "        grid[key][0,:,:]=0\n",
        "        grid[key][49,:,:]=0\n",
        "        grid[key][:,0,:]=0\n",
        "        grid[key][0,49,:]=0\n",
        "    \n",
        "    tmp = np.concatenate((grid[\"alldata\"]*255,grid[\"train\"]*255,grid[\"test\"]*255), axis=1)\n",
        "    tmp2 = np.concatenate((grid[\"pred\"],grid[\"train_error\"],grid[\"test_error\"]), axis=1)\n",
        "    tmp3 = np.concatenate((tmp,tmp2),axis=0)\n",
        "    return tmp3\n",
        "\n",
        "\n",
        "def train_test_deep_network(trainingdata,testingdata,model,nbIteration):\n",
        "    model.updateweights(trainingdata)\n",
        "    \n",
        "    displaysize = 20\n",
        "    fig, ax = plt.subplots(figsize=(displaysize, displaysize))\n",
        "    visu = visualize_current_model_behaviour(trainingdata,testingdata,model)\n",
        "    imgplot = ax.imshow(visu)\n",
        "    plt.ion()\n",
        "    plt.show()\n",
        "    plt.pause(0.001)\n",
        "\n",
        "    for iteration in range(nbIteration-1):\n",
        "        loss = model.updateweights(trainingdata)\n",
        "        \n",
        "        visu = visualize_current_model_behaviour(trainingdata,testingdata,model)\n",
        "        imgplot.set_data(visu)\n",
        "        plt.show()\n",
        "        plt.pause(0.001)\n",
        "\n",
        "    plt.pause(1)\n",
        "\n",
        " \n",
        "class Net(nn.Module):\n",
        "    def getPredictedClass(self,x):\n",
        "        variablex = torch.autograd.Variable(torch.Tensor(x.astype(float)))\n",
        "        variableoutput = self.forward(variablex)\n",
        "        prob = variableoutput.cpu().data.numpy()\n",
        "        return np.argmax(prob,axis=1)\n",
        "    \n",
        "    def updateweights(self,batchfromtrain):\n",
        "        x,y = batchfromtrain\n",
        "        variablex = torch.autograd.Variable(torch.Tensor(x.astype(float)))\n",
        "        variabley = torch.autograd.Variable(torch.from_numpy(y).long())\n",
        "        variableoutput = self.forward(variablex)\n",
        "        \n",
        "        loss = self.losslayer(variableoutput,variabley)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()        \n",
        "        return loss.cpu().data.numpy()\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 30, bias=True)\n",
        "        self.fc2 = nn.Linear(30, 30, bias=True)\n",
        "        self.fc2bis = nn.Linear(30, 30, bias=True)\n",
        "        self.fc3 = nn.Linear(30, 2, bias=True)\n",
        "        \n",
        "        self.train()\n",
        "        \n",
        "        \n",
        "        self.lr = 0.1\n",
        "        self.momentum = 0.5\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=self.lr, momentum=self.momentum)\n",
        "        self.losslayer = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x/30))\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = F.leaky_relu(self.fc2bis(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "        \n",
        "class NearestNeighbourg:\n",
        "    def getPredictedClass(self,x):\n",
        "        if len(x.shape)==1:        \n",
        "            distances = [(np.sum((x-self.X[i])*(x-self.X[i])),i) for i in range(self.X.shape[0])]\n",
        "            d,argmin = min(distances)\n",
        "            return self.Y[argmin]\n",
        "        else:\n",
        "            y = [self.getPredictedClass(x[i]) for i in range(x.shape[0])]\n",
        "            return np.asarray(y)\n",
        "    \n",
        "    def updateweights(self,x,y):\n",
        "        self.X = x\n",
        "        self.Y = y \n",
        "\n",
        "\n",
        "def data_from_grid_label(gridlabel):\n",
        "    X,Y = [],[]\n",
        "    tmp = np.zeros((1,2),dtype=int)\n",
        "    for row in range(10):\n",
        "        for col in range(10):\n",
        "            tmp[0][0]=row*5+2\n",
        "            tmp[0][1]=col*5+2\n",
        "            if gridlabel[row][col] == '+':\n",
        "                X.append(tmp.copy())\n",
        "                Y.append(1)\n",
        "            if gridlabel[row][col] == '-':\n",
        "                X.append(tmp.copy())\n",
        "                Y.append(0)\n",
        "    return np.concatenate(X,axis=0),np.asarray(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OThTjp_hNR4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def observe_network_behaviour_on_griddata(gridlabeltrain,gridlabeltest):\n",
        "    model = Net()\n",
        "    trainingdata,testingdata = data_from_grid_label(gridlabeltrain),data_from_grid_label(gridlabeltest)\n",
        "    train_test_deep_network(trainingdata,testingdata,model,300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUQcfvqCS62P",
        "colab_type": "text"
      },
      "source": [
        "## Exemples d'apprentissage :\n",
        "\n",
        "Faites tourner les 3 apprentissages ci dessous pour vous familiariser avec le code\n",
        "\n",
        "### CAS 1 : linéaire"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI37qd1cVyJD",
        "colab_type": "code",
        "outputId": "5a52faa8-6523-4b73-afe4-ab0fa612681e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train = [\n",
        "['-','-','-','-','-','-','-','-','-','-'],\n",
        "['-','-','-','-','-','-','-','-','-','-'],\n",
        "[' ',' ',' ',' ',' ',' ',' ',' ',' ',' '],\n",
        "[' ',' ',' ',' ',' ',' ',' ',' ',' ',' '],\n",
        "[' ',' ',' ',' ',' ',' ',' ',' ',' ',' '],\n",
        "[' ',' ',' ',' ',' ',' ',' ',' ',' ',' '],\n",
        "[' ',' ',' ',' ',' ',' ',' ',' ',' ',' '],\n",
        "[' ',' ',' ',' ',' ',' ',' ',' ',' ',' '],\n",
        "['+','+','+','+','+','+','+','+','+','+'],\n",
        "['+','+','+','+','+','+','+','+','+','+']]\n",
        "\n",
        "test = [\n",
        "[' ',' ',' ',' ',' ',' ',' ',' ',' ',' '],\n",
        "[' ',' ',' ',' ',' ',' ',' ',' ',' ',' '],\n",
        "[' ',' ',' ',' ',' ',' ',' ',' ',' ',' '],\n",
        "[' ',' ',' ',' ',' ',' ',' ',' ',' ',' '],\n",
        "['-','-','-','-','-','-','-','-','-','-'],\n",
        "['-','-','-','-','-','-','-','-','-','-'],\n",
        "['+','+','+','+','+','+','+','+','+','+'],\n",
        "['+','+','+','+','+','+','+','+','+','+'],\n",
        "[' ',' ',' ',' ',' ',' ',' ',' ',' ',' '],\n",
        "[' ',' ',' ',' ',' ',' ',' ',' ',' ',' ']]\n",
        "\n",
        "observe_network_behaviour_on_griddata(train,test)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIEAAAMKCAYAAAAS5lRUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5oBFIEmSJEmSpDlgEUiSJEmSJGkO/AsBJfrssKtd4AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cd225ac27819>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m [' ',' ',' ',' ',' ',' ',' ',' ',' ',' ']]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mobserve_network_behaviour_on_griddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-ebedc8046504>\u001b[0m in \u001b[0;36mobserve_network_behaviour_on_griddata\u001b[0;34m(gridlabeltrain, gridlabeltest)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrainingdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestingdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_from_grid_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgridlabeltrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_from_grid_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgridlabeltest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_test_deep_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestingdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-be9476170682>\u001b[0m in \u001b[0;36mtrain_test_deep_network\u001b[0;34m(trainingdata, testingdata, model, nbIteration)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mpause\u001b[0;34m(interval)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp88MHnQTXbK",
        "colab_type": "text"
      },
      "source": [
        "malheureusement de cette fonction on ne connait qu'un certain nombre d'échantillons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj_cta7CTWw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = np.random.randint(0,50, size=(50,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrTyBA5vTXAl",
        "colab_type": "text"
      },
      "source": [
        "et, à partir de ces échantillons seulement, on veut estimer la fonction (ce qui est impossible dans l'absolue mais qui peut marcher en pratique vue que les fonctions qu'on cherche à estimer son *régulière*).\n",
        "Pour cela on définit un réseau :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIAVxBupTSJk",
        "colab_type": "code",
        "outputId": "34de954b-9cbb-41ea-8e7c-cd690809dbf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 30, bias=True)\n",
        "        self.fc2 = nn.Linear(30, 30, bias=True)\n",
        "        self.fc2bis = nn.Linear(30, 30, bias=True)\n",
        "        self.fc3 = nn.Linear(30, 2, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x/30))\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = F.leaky_relu(self.fc2bis(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def forwardnp(self,x):\n",
        "        variablex = torch.autograd.Variable(torch.Tensor(x.astype(float)))\n",
        "        variableoutput = self.forward(variablex)\n",
        "        return variableoutput.cpu().data.numpy()\n",
        "\n",
        "model = Net()\n",
        "model.train()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (fc1): Linear(in_features=2, out_features=30, bias=True)\n",
              "  (fc2): Linear(in_features=30, out_features=30, bias=True)\n",
              "  (fc2bis): Linear(in_features=30, out_features=30, bias=True)\n",
              "  (fc3): Linear(in_features=30, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tAZm4eeTRl0",
        "colab_type": "text"
      },
      "source": [
        "et on va apprendre sur les échantillons\n",
        "\n",
        "ici juste quelques fonctions d'affichage "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN55wILyUCHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualizemodel(model,visufond,visusample):\n",
        "    grid = np.zeros((gridsize,gridsize,3),dtype=int)\n",
        "    grid[:]=255\n",
        "\n",
        "    batch = np.zeros((gridsize*gridsize,2),dtype=int)\n",
        "    for row in range(gridsize):\n",
        "        for col in range(gridsize):\n",
        "            batch[row*gridsize+col][0]=row\n",
        "            batch[row*gridsize+col][1]=col\n",
        "\n",
        "    prob = model.forwardnp(batch)\n",
        "    pred = np.argmax(prob,axis=1)\n",
        "\n",
        "    if visufond:\n",
        "        grid[:,:,0]=175\n",
        "        for row in range(gridsize):\n",
        "            for col in range(gridsize):\n",
        "                if pred[row*gridsize+col]==1:\n",
        "                    grid[row][col][1] = 175\n",
        "                else:\n",
        "                    grid[row][col][2] = 175\n",
        "\n",
        "    if visusample:\n",
        "        for row,col in samples:\n",
        "            grid[row][col][0]=0\n",
        "            if pred[row*gridsize+col]==1:\n",
        "                grid[row][col][1] = 0\n",
        "            else:\n",
        "                grid[row][col][2] = 0\n",
        "\n",
        "    return np.uint8(grid),pred\n",
        "\n",
        "def visualizeALL():\n",
        "    gt,gtpred = visualizemodel(groundtruth,True,False)\n",
        "    justsamples,_ = visualizemodel(groundtruth,False,True)\n",
        "    model.eval()\n",
        "    modelout,pred = visualizemodel(model,True,True)\n",
        "    model.train()\n",
        "    \n",
        "    samplemask = np.zeros(gridsize*gridsize,dtype=int)\n",
        "    for row,col in samples:\n",
        "        samplemask[row*gridsize+col] = 1\n",
        "    realerror = np.sum(np.absolute(gtpred-pred))\n",
        "    observederror = np.sum(np.absolute(gtpred-pred)*samplemask)\n",
        "    \n",
        "    return np.concatenate((gt,justsamples,modelout), axis=1),realerror,observederror\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enUj8a0rUW2g",
        "colab_type": "text"
      },
      "source": [
        "les paramètres de l'apprentissage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIxR_avk8hVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.1\n",
        "momentum = 0.5\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "losslayer = nn.CrossEntropyLoss()\n",
        "nbepoch = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rml7pp2SUbr2",
        "colab_type": "text"
      },
      "source": [
        "l'apprentissage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRvyY5yRRJGw",
        "colab_type": "code",
        "outputId": "d51d96f2-ab16-40fd-f26d-bfd09f8a15b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3672
        }
      },
      "source": [
        "from IPython.display import clear_output # command to clear the figures\n",
        "from time import sleep\n",
        "\n",
        "allprints = []\n",
        "for epoch in range(nbepoch):\n",
        "    batch = torch.autograd.Variable(torch.Tensor(samples.astype(float)).float())\n",
        "    target = torch.autograd.Variable(torch.from_numpy(np.argmax(groundtruth.forwardnp(samples),axis=1)).long())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(batch)\n",
        "\n",
        "    loss = losslayer(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "    grid,realerror,observederror = visualizeALL()\n",
        "    allprints.append((\"real error=\",realerror,\"\\tobserved error on samples=\", observederror, \"\\toptimisation loss=\", loss.cpu().data.numpy()))\n",
        "    \n",
        "    if epoch%8==0:\n",
        "        #show how it learn\n",
        "        clear_output()\n",
        "        plt.imshow(grid)\n",
        "        plt.show()\n",
        "        sleep(1)\n",
        "\n",
        "#print all the log at the end of the loop (otherwise you do not see the plot)\n",
        "for a,b,c,d,e,f in allprints:\n",
        "    print(a,b,c,d,e,f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAC2CAYAAACYjId+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGEbzBQDAMJovAACG/Q9ZKwThyzvZmQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.5872671\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.58501065\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.5826737\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.5802513\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.577759\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.5751996\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.5725545\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.5698205\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.5669856\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.5640572\n",
            "real error= 901 \tobserved error on samples= 17 \toptimisation loss= 0.56101984\n",
            "real error= 928 \tobserved error on samples= 17 \toptimisation loss= 0.5578815\n",
            "real error= 954 \tobserved error on samples= 17 \toptimisation loss= 0.55465835\n",
            "real error= 985 \tobserved error on samples= 17 \toptimisation loss= 0.5513342\n",
            "real error= 1006 \tobserved error on samples= 17 \toptimisation loss= 0.54789084\n",
            "real error= 1006 \tobserved error on samples= 16 \toptimisation loss= 0.54434836\n",
            "real error= 1001 \tobserved error on samples= 16 \toptimisation loss= 0.54072523\n",
            "real error= 991 \tobserved error on samples= 17 \toptimisation loss= 0.5369928\n",
            "real error= 982 \tobserved error on samples= 17 \toptimisation loss= 0.5331475\n",
            "real error= 970 \tobserved error on samples= 16 \toptimisation loss= 0.5291981\n",
            "real error= 957 \tobserved error on samples= 14 \toptimisation loss= 0.5251537\n",
            "real error= 943 \tobserved error on samples= 14 \toptimisation loss= 0.52102137\n",
            "real error= 932 \tobserved error on samples= 13 \toptimisation loss= 0.5168151\n",
            "real error= 919 \tobserved error on samples= 13 \toptimisation loss= 0.51252836\n",
            "real error= 907 \tobserved error on samples= 13 \toptimisation loss= 0.5081432\n",
            "real error= 889 \tobserved error on samples= 13 \toptimisation loss= 0.5036571\n",
            "real error= 875 \tobserved error on samples= 14 \toptimisation loss= 0.49898133\n",
            "real error= 860 \tobserved error on samples= 14 \toptimisation loss= 0.4941676\n",
            "real error= 845 \tobserved error on samples= 12 \toptimisation loss= 0.48923886\n",
            "real error= 832 \tobserved error on samples= 12 \toptimisation loss= 0.48423472\n",
            "real error= 818 \tobserved error on samples= 11 \toptimisation loss= 0.4791668\n",
            "real error= 803 \tobserved error on samples= 11 \toptimisation loss= 0.4739771\n",
            "real error= 789 \tobserved error on samples= 11 \toptimisation loss= 0.46867982\n",
            "real error= 781 \tobserved error on samples= 11 \toptimisation loss= 0.46328267\n",
            "real error= 767 \tobserved error on samples= 10 \toptimisation loss= 0.45788985\n",
            "real error= 755 \tobserved error on samples= 11 \toptimisation loss= 0.45239696\n",
            "real error= 744 \tobserved error on samples= 11 \toptimisation loss= 0.44682285\n",
            "real error= 739 \tobserved error on samples= 10 \toptimisation loss= 0.4411327\n",
            "real error= 718 \tobserved error on samples= 10 \toptimisation loss= 0.4352781\n",
            "real error= 709 \tobserved error on samples= 10 \toptimisation loss= 0.4292548\n",
            "real error= 700 \tobserved error on samples= 10 \toptimisation loss= 0.4230541\n",
            "real error= 690 \tobserved error on samples= 10 \toptimisation loss= 0.41675368\n",
            "real error= 677 \tobserved error on samples= 10 \toptimisation loss= 0.41032875\n",
            "real error= 667 \tobserved error on samples= 9 \toptimisation loss= 0.40384397\n",
            "real error= 657 \tobserved error on samples= 9 \toptimisation loss= 0.3972163\n",
            "real error= 645 \tobserved error on samples= 9 \toptimisation loss= 0.39056775\n",
            "real error= 634 \tobserved error on samples= 9 \toptimisation loss= 0.38385165\n",
            "real error= 624 \tobserved error on samples= 9 \toptimisation loss= 0.37701416\n",
            "real error= 615 \tobserved error on samples= 9 \toptimisation loss= 0.37013215\n",
            "real error= 611 \tobserved error on samples= 9 \toptimisation loss= 0.36319473\n",
            "real error= 595 \tobserved error on samples= 9 \toptimisation loss= 0.35617888\n",
            "real error= 582 \tobserved error on samples= 9 \toptimisation loss= 0.3490517\n",
            "real error= 575 \tobserved error on samples= 9 \toptimisation loss= 0.34201545\n",
            "real error= 574 \tobserved error on samples= 9 \toptimisation loss= 0.33512372\n",
            "real error= 569 \tobserved error on samples= 9 \toptimisation loss= 0.32826722\n",
            "real error= 558 \tobserved error on samples= 9 \toptimisation loss= 0.32146278\n",
            "real error= 551 \tobserved error on samples= 8 \toptimisation loss= 0.314826\n",
            "real error= 547 \tobserved error on samples= 8 \toptimisation loss= 0.30839378\n",
            "real error= 543 \tobserved error on samples= 8 \toptimisation loss= 0.30209908\n",
            "real error= 531 \tobserved error on samples= 7 \toptimisation loss= 0.29585797\n",
            "real error= 524 \tobserved error on samples= 7 \toptimisation loss= 0.2897079\n",
            "real error= 519 \tobserved error on samples= 7 \toptimisation loss= 0.28370503\n",
            "real error= 513 \tobserved error on samples= 7 \toptimisation loss= 0.27784085\n",
            "real error= 502 \tobserved error on samples= 7 \toptimisation loss= 0.27205995\n",
            "real error= 500 \tobserved error on samples= 6 \toptimisation loss= 0.26644444\n",
            "real error= 496 \tobserved error on samples= 6 \toptimisation loss= 0.26101306\n",
            "real error= 495 \tobserved error on samples= 6 \toptimisation loss= 0.2557217\n",
            "real error= 489 \tobserved error on samples= 6 \toptimisation loss= 0.25033155\n",
            "real error= 487 \tobserved error on samples= 6 \toptimisation loss= 0.24514598\n",
            "real error= 487 \tobserved error on samples= 5 \toptimisation loss= 0.24001713\n",
            "real error= 485 \tobserved error on samples= 4 \toptimisation loss= 0.23509654\n",
            "real error= 479 \tobserved error on samples= 4 \toptimisation loss= 0.23038018\n",
            "real error= 478 \tobserved error on samples= 4 \toptimisation loss= 0.2258796\n",
            "real error= 477 \tobserved error on samples= 4 \toptimisation loss= 0.2215404\n",
            "real error= 474 \tobserved error on samples= 4 \toptimisation loss= 0.21740925\n",
            "real error= 475 \tobserved error on samples= 4 \toptimisation loss= 0.21344626\n",
            "real error= 470 \tobserved error on samples= 3 \toptimisation loss= 0.20963871\n",
            "real error= 467 \tobserved error on samples= 3 \toptimisation loss= 0.20598505\n",
            "real error= 466 \tobserved error on samples= 3 \toptimisation loss= 0.20247754\n",
            "real error= 468 \tobserved error on samples= 3 \toptimisation loss= 0.19909382\n",
            "real error= 466 \tobserved error on samples= 3 \toptimisation loss= 0.19582736\n",
            "real error= 466 \tobserved error on samples= 3 \toptimisation loss= 0.19265248\n",
            "real error= 469 \tobserved error on samples= 3 \toptimisation loss= 0.1895786\n",
            "real error= 465 \tobserved error on samples= 3 \toptimisation loss= 0.18662795\n",
            "real error= 457 \tobserved error on samples= 3 \toptimisation loss= 0.18374598\n",
            "real error= 457 \tobserved error on samples= 3 \toptimisation loss= 0.18091877\n",
            "real error= 458 \tobserved error on samples= 3 \toptimisation loss= 0.1781442\n",
            "real error= 458 \tobserved error on samples= 3 \toptimisation loss= 0.17541979\n",
            "real error= 459 \tobserved error on samples= 3 \toptimisation loss= 0.1728355\n",
            "real error= 458 \tobserved error on samples= 3 \toptimisation loss= 0.17031881\n",
            "real error= 457 \tobserved error on samples= 3 \toptimisation loss= 0.16788131\n",
            "real error= 455 \tobserved error on samples= 3 \toptimisation loss= 0.16549028\n",
            "real error= 458 \tobserved error on samples= 3 \toptimisation loss= 0.16311824\n",
            "real error= 457 \tobserved error on samples= 3 \toptimisation loss= 0.16077234\n",
            "real error= 455 \tobserved error on samples= 3 \toptimisation loss= 0.15849054\n",
            "real error= 454 \tobserved error on samples= 3 \toptimisation loss= 0.15622444\n",
            "real error= 454 \tobserved error on samples= 3 \toptimisation loss= 0.15398888\n",
            "real error= 453 \tobserved error on samples= 3 \toptimisation loss= 0.15179965\n",
            "real error= 451 \tobserved error on samples= 3 \toptimisation loss= 0.14965643\n",
            "real error= 449 \tobserved error on samples= 3 \toptimisation loss= 0.14755976\n",
            "real error= 449 \tobserved error on samples= 3 \toptimisation loss= 0.1454453\n",
            "real error= 446 \tobserved error on samples= 2 \toptimisation loss= 0.14339496\n",
            "real error= 450 \tobserved error on samples= 2 \toptimisation loss= 0.14123036\n",
            "real error= 443 \tobserved error on samples= 2 \toptimisation loss= 0.13911699\n",
            "real error= 442 \tobserved error on samples= 2 \toptimisation loss= 0.13697201\n",
            "real error= 441 \tobserved error on samples= 2 \toptimisation loss= 0.13490361\n",
            "real error= 438 \tobserved error on samples= 2 \toptimisation loss= 0.13277562\n",
            "real error= 437 \tobserved error on samples= 2 \toptimisation loss= 0.13070078\n",
            "real error= 434 \tobserved error on samples= 2 \toptimisation loss= 0.12860076\n",
            "real error= 434 \tobserved error on samples= 2 \toptimisation loss= 0.12666047\n",
            "real error= 433 \tobserved error on samples= 2 \toptimisation loss= 0.12473772\n",
            "real error= 430 \tobserved error on samples= 2 \toptimisation loss= 0.122878335\n",
            "real error= 431 \tobserved error on samples= 2 \toptimisation loss= 0.12099109\n",
            "real error= 428 \tobserved error on samples= 2 \toptimisation loss= 0.119173184\n",
            "real error= 425 \tobserved error on samples= 2 \toptimisation loss= 0.117314756\n",
            "real error= 423 \tobserved error on samples= 2 \toptimisation loss= 0.11547862\n",
            "real error= 419 \tobserved error on samples= 2 \toptimisation loss= 0.11380745\n",
            "real error= 421 \tobserved error on samples= 2 \toptimisation loss= 0.11204986\n",
            "real error= 421 \tobserved error on samples= 2 \toptimisation loss= 0.110355355\n",
            "real error= 416 \tobserved error on samples= 2 \toptimisation loss= 0.10862868\n",
            "real error= 417 \tobserved error on samples= 2 \toptimisation loss= 0.10701953\n",
            "real error= 414 \tobserved error on samples= 2 \toptimisation loss= 0.10529074\n",
            "real error= 412 \tobserved error on samples= 2 \toptimisation loss= 0.10377222\n",
            "real error= 414 \tobserved error on samples= 2 \toptimisation loss= 0.10216645\n",
            "real error= 413 \tobserved error on samples= 2 \toptimisation loss= 0.100515425\n",
            "real error= 410 \tobserved error on samples= 2 \toptimisation loss= 0.099003404\n",
            "real error= 408 \tobserved error on samples= 2 \toptimisation loss= 0.0975198\n",
            "real error= 408 \tobserved error on samples= 2 \toptimisation loss= 0.09599573\n",
            "real error= 408 \tobserved error on samples= 2 \toptimisation loss= 0.09453733\n",
            "real error= 406 \tobserved error on samples= 2 \toptimisation loss= 0.09333052\n",
            "real error= 403 \tobserved error on samples= 1 \toptimisation loss= 0.09175955\n",
            "real error= 403 \tobserved error on samples= 1 \toptimisation loss= 0.09036111\n",
            "real error= 399 \tobserved error on samples= 1 \toptimisation loss= 0.08908664\n",
            "real error= 397 \tobserved error on samples= 1 \toptimisation loss= 0.087877944\n",
            "real error= 393 \tobserved error on samples= 1 \toptimisation loss= 0.08669077\n",
            "real error= 395 \tobserved error on samples= 1 \toptimisation loss= 0.08528117\n",
            "real error= 392 \tobserved error on samples= 1 \toptimisation loss= 0.08401176\n",
            "real error= 393 \tobserved error on samples= 1 \toptimisation loss= 0.082785465\n",
            "real error= 392 \tobserved error on samples= 1 \toptimisation loss= 0.081534036\n",
            "real error= 391 \tobserved error on samples= 1 \toptimisation loss= 0.080415994\n",
            "real error= 389 \tobserved error on samples= 1 \toptimisation loss= 0.07927814\n",
            "real error= 392 \tobserved error on samples= 1 \toptimisation loss= 0.078213185\n",
            "real error= 389 \tobserved error on samples= 1 \toptimisation loss= 0.077112556\n",
            "real error= 388 \tobserved error on samples= 1 \toptimisation loss= 0.07559981\n",
            "real error= 387 \tobserved error on samples= 1 \toptimisation loss= 0.07466836\n",
            "real error= 387 \tobserved error on samples= 1 \toptimisation loss= 0.0733526\n",
            "real error= 384 \tobserved error on samples= 1 \toptimisation loss= 0.072173625\n",
            "real error= 380 \tobserved error on samples= 1 \toptimisation loss= 0.0711437\n",
            "real error= 378 \tobserved error on samples= 1 \toptimisation loss= 0.07074194\n",
            "real error= 376 \tobserved error on samples= 1 \toptimisation loss= 0.06933881\n",
            "real error= 379 \tobserved error on samples= 1 \toptimisation loss= 0.06836093\n",
            "real error= 375 \tobserved error on samples= 1 \toptimisation loss= 0.067390606\n",
            "real error= 375 \tobserved error on samples= 1 \toptimisation loss= 0.06646327\n",
            "real error= 371 \tobserved error on samples= 1 \toptimisation loss= 0.065152384\n",
            "real error= 372 \tobserved error on samples= 1 \toptimisation loss= 0.06412248\n",
            "real error= 374 \tobserved error on samples= 1 \toptimisation loss= 0.06328802\n",
            "real error= 370 \tobserved error on samples= 1 \toptimisation loss= 0.062458154\n",
            "real error= 372 \tobserved error on samples= 1 \toptimisation loss= 0.061750706\n",
            "real error= 367 \tobserved error on samples= 1 \toptimisation loss= 0.060992014\n",
            "real error= 371 \tobserved error on samples= 1 \toptimisation loss= 0.060659353\n",
            "real error= 368 \tobserved error on samples= 1 \toptimisation loss= 0.05968543\n",
            "real error= 369 \tobserved error on samples= 1 \toptimisation loss= 0.0588449\n",
            "real error= 364 \tobserved error on samples= 1 \toptimisation loss= 0.05786279\n",
            "real error= 369 \tobserved error on samples= 1 \toptimisation loss= 0.057127498\n",
            "real error= 361 \tobserved error on samples= 1 \toptimisation loss= 0.05602377\n",
            "real error= 364 \tobserved error on samples= 1 \toptimisation loss= 0.05519887\n",
            "real error= 360 \tobserved error on samples= 1 \toptimisation loss= 0.054261494\n",
            "real error= 365 \tobserved error on samples= 0 \toptimisation loss= 0.053970903\n",
            "real error= 354 \tobserved error on samples= 1 \toptimisation loss= 0.05348218\n",
            "real error= 357 \tobserved error on samples= 0 \toptimisation loss= 0.053231526\n",
            "real error= 356 \tobserved error on samples= 1 \toptimisation loss= 0.05211415\n",
            "real error= 360 \tobserved error on samples= 0 \toptimisation loss= 0.051599815\n",
            "real error= 352 \tobserved error on samples= 1 \toptimisation loss= 0.05092924\n",
            "real error= 358 \tobserved error on samples= 0 \toptimisation loss= 0.050160658\n",
            "real error= 353 \tobserved error on samples= 1 \toptimisation loss= 0.049468298\n",
            "real error= 356 \tobserved error on samples= 1 \toptimisation loss= 0.04836546\n",
            "real error= 353 \tobserved error on samples= 1 \toptimisation loss= 0.047775075\n",
            "real error= 351 \tobserved error on samples= 1 \toptimisation loss= 0.046969425\n",
            "real error= 353 \tobserved error on samples= 0 \toptimisation loss= 0.04665431\n",
            "real error= 350 \tobserved error on samples= 1 \toptimisation loss= 0.045979377\n",
            "real error= 355 \tobserved error on samples= 0 \toptimisation loss= 0.045602113\n",
            "real error= 346 \tobserved error on samples= 1 \toptimisation loss= 0.045598127\n",
            "real error= 360 \tobserved error on samples= 0 \toptimisation loss= 0.045278504\n",
            "real error= 349 \tobserved error on samples= 1 \toptimisation loss= 0.04578064\n",
            "real error= 354 \tobserved error on samples= 0 \toptimisation loss= 0.04482249\n",
            "real error= 344 \tobserved error on samples= 1 \toptimisation loss= 0.043536354\n",
            "real error= 347 \tobserved error on samples= 0 \toptimisation loss= 0.04315531\n",
            "real error= 346 \tobserved error on samples= 0 \toptimisation loss= 0.042095777\n",
            "real error= 343 \tobserved error on samples= 1 \toptimisation loss= 0.041283198\n",
            "real error= 351 \tobserved error on samples= 0 \toptimisation loss= 0.04085166\n",
            "real error= 342 \tobserved error on samples= 1 \toptimisation loss= 0.040973097\n",
            "real error= 350 \tobserved error on samples= 0 \toptimisation loss= 0.04084386\n",
            "real error= 343 \tobserved error on samples= 1 \toptimisation loss= 0.03991651\n",
            "real error= 347 \tobserved error on samples= 0 \toptimisation loss= 0.03932127\n",
            "real error= 340 \tobserved error on samples= 1 \toptimisation loss= 0.038831953\n",
            "real error= 343 \tobserved error on samples= 0 \toptimisation loss= 0.038532242\n",
            "real error= 337 \tobserved error on samples= 0 \toptimisation loss= 0.037871167\n",
            "real error= 346 \tobserved error on samples= 0 \toptimisation loss= 0.03708992\n",
            "real error= 334 \tobserved error on samples= 0 \toptimisation loss= 0.03736773\n",
            "real error= 339 \tobserved error on samples= 0 \toptimisation loss= 0.036670014\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}