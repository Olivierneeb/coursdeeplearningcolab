{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achanhon/coursdeeplearningcolab/blob/master/tpnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "pI37qd1cVyJD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3840
        },
        "outputId": "e4e3a678-09f9-477b-a3f6-e811f86effe6"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.autograd\n",
        "import torch.autograd.variable\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import animation\n",
        "\n",
        "gridsize = 50\n",
        "\n",
        "class GroundTruth(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GroundTruth, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 4, bias=True)\n",
        "        self.fc2 = nn.Linear(4, 1, bias=True)\n",
        "        self.fc3 = nn.Linear(1, 2, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def forwardnp(self,x):\n",
        "        variablex = torch.autograd.Variable(torch.Tensor(x.astype(float)))\n",
        "        variableoutput = self.forward(variablex)\n",
        "        return variableoutput.cpu().data.numpy()\n",
        "\n",
        "groundtruth = GroundTruth()\n",
        "groundtruth.fc1.weight.data[0][0] = -1\n",
        "groundtruth.fc1.weight.data[0][1] = 0\n",
        "groundtruth.fc1.bias.data[0] = 10\n",
        "groundtruth.fc1.weight.data[1][0] = 1\n",
        "groundtruth.fc1.weight.data[1][1] = 0\n",
        "groundtruth.fc1.bias.data[1] = -30\n",
        "groundtruth.fc1.weight.data[2][0] = 0\n",
        "groundtruth.fc1.weight.data[2][1] = -1\n",
        "groundtruth.fc1.bias.data[2] = 10\n",
        "groundtruth.fc1.weight.data[3][0] = 0\n",
        "groundtruth.fc1.weight.data[3][1] = 1\n",
        "groundtruth.fc1.bias.data[3] = -30\n",
        "\n",
        "groundtruth.fc2.bias.data[0] = 3\n",
        "groundtruth.fc2.weight.data[0][0] = -0.5\n",
        "groundtruth.fc2.weight.data[0][1] = -0.5\n",
        "groundtruth.fc2.weight.data[0][2] = -0.5\n",
        "groundtruth.fc2.weight.data[0][3] = -0.5\n",
        "\n",
        "groundtruth.fc3.bias.data[0] = -0.5\n",
        "groundtruth.fc3.weight.data[0][0] = 1\n",
        "groundtruth.fc3.weight.data[1] = -groundtruth.fc3.weight.data[0]\n",
        "groundtruth.fc3.bias.data[1] = -groundtruth.fc3.bias.data[0]\n",
        "groundtruth.eval()\n",
        "\n",
        "samples = np.random.randint(0,50, size=(50,2))\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 30, bias=True)\n",
        "        self.fc2 = nn.Linear(30, 30, bias=True)\n",
        "        self.fc2bis = nn.Linear(30, 30, bias=True)\n",
        "        self.fc3 = nn.Linear(30, 2, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x/30))\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = F.leaky_relu(self.fc2bis(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def forwardnp(self,x):\n",
        "        variablex = torch.autograd.Variable(torch.Tensor(x.astype(float)))\n",
        "        variableoutput = self.forward(variablex)\n",
        "        return variableoutput.cpu().data.numpy()\n",
        "\n",
        "model = Net()\n",
        "model.train()\n",
        "\n",
        "def visualizemodel(model,visufond,visusample):\n",
        "    grid = np.zeros((gridsize,gridsize,3),dtype=int)\n",
        "    grid[:]=255\n",
        "\n",
        "    batch = np.zeros((gridsize*gridsize,2),dtype=int)\n",
        "    for row in range(gridsize):\n",
        "        for col in range(gridsize):\n",
        "            batch[row*gridsize+col][0]=row\n",
        "            batch[row*gridsize+col][1]=col\n",
        "\n",
        "    prob = model.forwardnp(batch)\n",
        "    pred = np.argmax(prob,axis=1)\n",
        "\n",
        "    if visufond:\n",
        "        grid[:,:,0]=175\n",
        "        for row in range(gridsize):\n",
        "            for col in range(gridsize):\n",
        "                if pred[row*gridsize+col]==1:\n",
        "                    grid[row][col][1] = 175\n",
        "                else:\n",
        "                    grid[row][col][2] = 175\n",
        "\n",
        "    if visusample:\n",
        "        for row,col in samples:\n",
        "            grid[row][col][0]=0\n",
        "            if pred[row*gridsize+col]==1:\n",
        "                grid[row][col][1] = 0\n",
        "            else:\n",
        "                grid[row][col][2] = 0\n",
        "\n",
        "    return np.uint8(grid),pred\n",
        "\n",
        "def visualizeALL():\n",
        "    gt,gtpred = visualizemodel(groundtruth,True,False)\n",
        "    justsamples,_ = visualizemodel(groundtruth,False,True)\n",
        "    model.eval()\n",
        "    modelout,pred = visualizemodel(model,True,True)\n",
        "    model.train()\n",
        "    \n",
        "    samplemask = np.zeros(gridsize*gridsize,dtype=int)\n",
        "    for row,col in samples:\n",
        "        samplemask[row*gridsize+col] = 1\n",
        "    realerror = np.sum(np.absolute(gtpred-pred))\n",
        "    observederror = np.sum(np.absolute(gtpred-pred)*samplemask)\n",
        "    \n",
        "    return np.concatenate((gt,justsamples,modelout), axis=1),realerror,observederror\n",
        "\n",
        "lr = 0.1\n",
        "momentum = 0.5\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "losslayer = nn.CrossEntropyLoss()\n",
        "\n",
        "displaysize = 200\n",
        "fig, ax = plt.subplots(figsize=(displaysize*3/100*2, displaysize/100*2))\n",
        "grid,realerror,observederror = visualizeALL()\n",
        "imgplot = ax.imshow(grid)\n",
        "plt.ion()\n",
        "plt.show()\n",
        "plt.pause(0.001)\n",
        "\n",
        "nbepoch = 200\n",
        "for epoch in range(nbepoch):\n",
        "    batch = torch.autograd.Variable(torch.Tensor(samples.astype(float)).float())\n",
        "    target = torch.autograd.Variable(torch.from_numpy(np.argmax(groundtruth.forwardnp(samples),axis=1)).long())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(batch)\n",
        "\n",
        "    loss = losslayer(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    grid,realerror,observederror = visualizeALL()\n",
        "    print(\"real error=\",realerror,\"\\tobserved error on samples=\", observederror, \"\\toptimisation loss=\", loss.cpu().data.numpy())\n",
        "    \n",
        "    imgplot.set_data(grid)\n",
        "    plt.show()\n",
        "    plt.pause(0.001)\n",
        "\n",
        "plt.pause(0)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqwAAAD6CAYAAABkihXzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFQCAogmsAAAUTWAFAKBo/x/FOTM0KAX8wgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.69113153\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6888548\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6864232\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.68431795\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6824951\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.68092936\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.67959666\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6783922\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6772738\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6762005\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.67518616\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6741907\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6732034\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.67223984\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6712894\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.67032534\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.66935027\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.668354\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6673276\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6662251\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6650947\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6639761\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.66282815\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6616699\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6605173\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6593349\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6581409\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6569494\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.65573174\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6544625\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.65315294\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6518084\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.65043485\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6490136\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6475471\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.64604485\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6445034\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.64291596\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.64127153\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.63956773\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6377936\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6359707\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.634059\n",
            "real error= 901 \tobserved error on samples= 21 \toptimisation loss= 0.6320703\n",
            "real error= 918 \tobserved error on samples= 21 \toptimisation loss= 0.62994933\n",
            "real error= 960 \tobserved error on samples= 22 \toptimisation loss= 0.6277063\n",
            "real error= 1003 \tobserved error on samples= 22 \toptimisation loss= 0.62531817\n",
            "real error= 1010 \tobserved error on samples= 23 \toptimisation loss= 0.6228478\n",
            "real error= 993 \tobserved error on samples= 24 \toptimisation loss= 0.62028605\n",
            "real error= 972 \tobserved error on samples= 22 \toptimisation loss= 0.6177055\n",
            "real error= 952 \tobserved error on samples= 22 \toptimisation loss= 0.61508757\n",
            "real error= 938 \tobserved error on samples= 19 \toptimisation loss= 0.61242425\n",
            "real error= 929 \tobserved error on samples= 18 \toptimisation loss= 0.60978085\n",
            "real error= 921 \tobserved error on samples= 16 \toptimisation loss= 0.6071149\n",
            "real error= 919 \tobserved error on samples= 17 \toptimisation loss= 0.60434604\n",
            "real error= 924 \tobserved error on samples= 17 \toptimisation loss= 0.6015461\n",
            "real error= 923 \tobserved error on samples= 17 \toptimisation loss= 0.59879214\n",
            "real error= 932 \tobserved error on samples= 17 \toptimisation loss= 0.5960304\n",
            "real error= 935 \tobserved error on samples= 17 \toptimisation loss= 0.59325904\n",
            "real error= 941 \tobserved error on samples= 19 \toptimisation loss= 0.5904889\n",
            "real error= 948 \tobserved error on samples= 18 \toptimisation loss= 0.5877534\n",
            "real error= 947 \tobserved error on samples= 18 \toptimisation loss= 0.5849767\n",
            "real error= 949 \tobserved error on samples= 19 \toptimisation loss= 0.58214104\n",
            "real error= 948 \tobserved error on samples= 19 \toptimisation loss= 0.5793469\n",
            "real error= 946 \tobserved error on samples= 19 \toptimisation loss= 0.576559\n",
            "real error= 945 \tobserved error on samples= 20 \toptimisation loss= 0.57376665\n",
            "real error= 943 \tobserved error on samples= 20 \toptimisation loss= 0.571005\n",
            "real error= 942 \tobserved error on samples= 20 \toptimisation loss= 0.56832814\n",
            "real error= 939 \tobserved error on samples= 20 \toptimisation loss= 0.5657099\n",
            "real error= 938 \tobserved error on samples= 20 \toptimisation loss= 0.56311923\n",
            "real error= 936 \tobserved error on samples= 20 \toptimisation loss= 0.56054425\n",
            "real error= 933 \tobserved error on samples= 18 \toptimisation loss= 0.5580284\n",
            "real error= 931 \tobserved error on samples= 18 \toptimisation loss= 0.55556375\n",
            "real error= 930 \tobserved error on samples= 17 \toptimisation loss= 0.5531454\n",
            "real error= 929 \tobserved error on samples= 17 \toptimisation loss= 0.55077577\n",
            "real error= 930 \tobserved error on samples= 17 \toptimisation loss= 0.548391\n",
            "real error= 920 \tobserved error on samples= 17 \toptimisation loss= 0.54599154\n",
            "real error= 917 \tobserved error on samples= 17 \toptimisation loss= 0.54352903\n",
            "real error= 917 \tobserved error on samples= 17 \toptimisation loss= 0.5409622\n",
            "real error= 908 \tobserved error on samples= 16 \toptimisation loss= 0.538371\n",
            "real error= 904 \tobserved error on samples= 16 \toptimisation loss= 0.5359072\n",
            "real error= 899 \tobserved error on samples= 16 \toptimisation loss= 0.5334396\n",
            "real error= 898 \tobserved error on samples= 16 \toptimisation loss= 0.5309033\n",
            "real error= 891 \tobserved error on samples= 16 \toptimisation loss= 0.52847165\n",
            "real error= 892 \tobserved error on samples= 16 \toptimisation loss= 0.52596205\n",
            "real error= 885 \tobserved error on samples= 16 \toptimisation loss= 0.5234113\n",
            "real error= 883 \tobserved error on samples= 16 \toptimisation loss= 0.52086276\n",
            "real error= 882 \tobserved error on samples= 16 \toptimisation loss= 0.51830035\n",
            "real error= 882 \tobserved error on samples= 16 \toptimisation loss= 0.5156734\n",
            "real error= 875 \tobserved error on samples= 15 \toptimisation loss= 0.51302296\n",
            "real error= 872 \tobserved error on samples= 15 \toptimisation loss= 0.51034945\n",
            "real error= 866 \tobserved error on samples= 14 \toptimisation loss= 0.5076256\n",
            "real error= 862 \tobserved error on samples= 13 \toptimisation loss= 0.50478315\n",
            "real error= 861 \tobserved error on samples= 13 \toptimisation loss= 0.50192934\n",
            "real error= 860 \tobserved error on samples= 13 \toptimisation loss= 0.49903065\n",
            "real error= 858 \tobserved error on samples= 13 \toptimisation loss= 0.49613258\n",
            "real error= 857 \tobserved error on samples= 13 \toptimisation loss= 0.49315396\n",
            "real error= 849 \tobserved error on samples= 12 \toptimisation loss= 0.49012515\n",
            "real error= 840 \tobserved error on samples= 12 \toptimisation loss= 0.4869247\n",
            "real error= 830 \tobserved error on samples= 11 \toptimisation loss= 0.4835159\n",
            "real error= 812 \tobserved error on samples= 10 \toptimisation loss= 0.47987682\n",
            "real error= 789 \tobserved error on samples= 9 \toptimisation loss= 0.47605526\n",
            "real error= 775 \tobserved error on samples= 9 \toptimisation loss= 0.47217137\n",
            "real error= 749 \tobserved error on samples= 9 \toptimisation loss= 0.4681608\n",
            "real error= 726 \tobserved error on samples= 9 \toptimisation loss= 0.46399057\n",
            "real error= 702 \tobserved error on samples= 9 \toptimisation loss= 0.45947793\n",
            "real error= 678 \tobserved error on samples= 9 \toptimisation loss= 0.45456883\n",
            "real error= 656 \tobserved error on samples= 8 \toptimisation loss= 0.4494941\n",
            "real error= 628 \tobserved error on samples= 8 \toptimisation loss= 0.44426018\n",
            "real error= 615 \tobserved error on samples= 8 \toptimisation loss= 0.43883696\n",
            "real error= 596 \tobserved error on samples= 8 \toptimisation loss= 0.43314174\n",
            "real error= 572 \tobserved error on samples= 8 \toptimisation loss= 0.42733002\n",
            "real error= 552 \tobserved error on samples= 8 \toptimisation loss= 0.42131782\n",
            "real error= 526 \tobserved error on samples= 8 \toptimisation loss= 0.4151723\n",
            "real error= 515 \tobserved error on samples= 8 \toptimisation loss= 0.40885487\n",
            "real error= 494 \tobserved error on samples= 7 \toptimisation loss= 0.40234673\n",
            "real error= 475 \tobserved error on samples= 7 \toptimisation loss= 0.3956423\n",
            "real error= 447 \tobserved error on samples= 7 \toptimisation loss= 0.3887393\n",
            "real error= 428 \tobserved error on samples= 7 \toptimisation loss= 0.3816778\n",
            "real error= 412 \tobserved error on samples= 7 \toptimisation loss= 0.3747145\n",
            "real error= 388 \tobserved error on samples= 6 \toptimisation loss= 0.36722097\n",
            "real error= 367 \tobserved error on samples= 6 \toptimisation loss= 0.35972664\n",
            "real error= 355 \tobserved error on samples= 6 \toptimisation loss= 0.35221124\n",
            "real error= 345 \tobserved error on samples= 6 \toptimisation loss= 0.34419152\n",
            "real error= 324 \tobserved error on samples= 4 \toptimisation loss= 0.33617574\n",
            "real error= 307 \tobserved error on samples= 5 \toptimisation loss= 0.32811725\n",
            "real error= 280 \tobserved error on samples= 3 \toptimisation loss= 0.31968352\n",
            "real error= 266 \tobserved error on samples= 2 \toptimisation loss= 0.31150922\n",
            "real error= 258 \tobserved error on samples= 2 \toptimisation loss= 0.303105\n",
            "real error= 251 \tobserved error on samples= 2 \toptimisation loss= 0.29515484\n",
            "real error= 233 \tobserved error on samples= 1 \toptimisation loss= 0.2870367\n",
            "real error= 218 \tobserved error on samples= 1 \toptimisation loss= 0.2794314\n",
            "real error= 210 \tobserved error on samples= 1 \toptimisation loss= 0.27084592\n",
            "real error= 207 \tobserved error on samples= 1 \toptimisation loss= 0.26321214\n",
            "real error= 208 \tobserved error on samples= 1 \toptimisation loss= 0.2555973\n",
            "real error= 184 \tobserved error on samples= 1 \toptimisation loss= 0.24807227\n",
            "real error= 190 \tobserved error on samples= 1 \toptimisation loss= 0.24006993\n",
            "real error= 181 \tobserved error on samples= 1 \toptimisation loss= 0.23267536\n",
            "real error= 178 \tobserved error on samples= 1 \toptimisation loss= 0.22523378\n",
            "real error= 175 \tobserved error on samples= 0 \toptimisation loss= 0.2182235\n",
            "real error= 166 \tobserved error on samples= 0 \toptimisation loss= 0.21124807\n",
            "real error= 170 \tobserved error on samples= 0 \toptimisation loss= 0.20482165\n",
            "real error= 160 \tobserved error on samples= 0 \toptimisation loss= 0.1983816\n",
            "real error= 167 \tobserved error on samples= 0 \toptimisation loss= 0.19243287\n",
            "real error= 155 \tobserved error on samples= 0 \toptimisation loss= 0.18624443\n",
            "real error= 146 \tobserved error on samples= 0 \toptimisation loss= 0.18001942\n",
            "real error= 148 \tobserved error on samples= 0 \toptimisation loss= 0.17397131\n",
            "real error= 146 \tobserved error on samples= 0 \toptimisation loss= 0.16825737\n",
            "real error= 146 \tobserved error on samples= 0 \toptimisation loss= 0.16293082\n",
            "real error= 140 \tobserved error on samples= 0 \toptimisation loss= 0.1577374\n",
            "real error= 156 \tobserved error on samples= 0 \toptimisation loss= 0.15291491\n",
            "real error= 140 \tobserved error on samples= 0 \toptimisation loss= 0.14819612\n",
            "real error= 142 \tobserved error on samples= 0 \toptimisation loss= 0.14307833\n",
            "real error= 142 \tobserved error on samples= 0 \toptimisation loss= 0.13863385\n",
            "real error= 144 \tobserved error on samples= 0 \toptimisation loss= 0.13400957\n",
            "real error= 143 \tobserved error on samples= 0 \toptimisation loss= 0.12985362\n",
            "real error= 146 \tobserved error on samples= 0 \toptimisation loss= 0.12564777\n",
            "real error= 144 \tobserved error on samples= 0 \toptimisation loss= 0.12182171\n",
            "real error= 158 \tobserved error on samples= 0 \toptimisation loss= 0.11814734\n",
            "real error= 145 \tobserved error on samples= 0 \toptimisation loss= 0.11460427\n",
            "real error= 164 \tobserved error on samples= 0 \toptimisation loss= 0.11139352\n",
            "real error= 149 \tobserved error on samples= 0 \toptimisation loss= 0.107993625\n",
            "real error= 165 \tobserved error on samples= 0 \toptimisation loss= 0.10495553\n",
            "real error= 150 \tobserved error on samples= 0 \toptimisation loss= 0.10161056\n",
            "real error= 156 \tobserved error on samples= 0 \toptimisation loss= 0.09817585\n",
            "real error= 161 \tobserved error on samples= 0 \toptimisation loss= 0.095143184\n",
            "real error= 154 \tobserved error on samples= 0 \toptimisation loss= 0.09254059\n",
            "real error= 168 \tobserved error on samples= 0 \toptimisation loss= 0.09012228\n",
            "real error= 156 \tobserved error on samples= 0 \toptimisation loss= 0.08773008\n",
            "real error= 166 \tobserved error on samples= 0 \toptimisation loss= 0.08516014\n",
            "real error= 167 \tobserved error on samples= 0 \toptimisation loss= 0.082772866\n",
            "real error= 156 \tobserved error on samples= 0 \toptimisation loss= 0.080599405\n",
            "real error= 163 \tobserved error on samples= 0 \toptimisation loss= 0.07848988\n",
            "real error= 158 \tobserved error on samples= 0 \toptimisation loss= 0.076456234\n",
            "real error= 150 \tobserved error on samples= 0 \toptimisation loss= 0.07450714\n",
            "real error= 171 \tobserved error on samples= 0 \toptimisation loss= 0.07275756\n",
            "real error= 153 \tobserved error on samples= 0 \toptimisation loss= 0.07097605\n",
            "real error= 167 \tobserved error on samples= 0 \toptimisation loss= 0.069131024\n",
            "real error= 165 \tobserved error on samples= 0 \toptimisation loss= 0.06741604\n",
            "real error= 157 \tobserved error on samples= 0 \toptimisation loss= 0.06578297\n",
            "real error= 158 \tobserved error on samples= 0 \toptimisation loss= 0.064210206\n",
            "real error= 152 \tobserved error on samples= 0 \toptimisation loss= 0.06270588\n",
            "real error= 169 \tobserved error on samples= 0 \toptimisation loss= 0.061330404\n",
            "real error= 156 \tobserved error on samples= 0 \toptimisation loss= 0.059887532\n",
            "real error= 157 \tobserved error on samples= 0 \toptimisation loss= 0.058433566\n",
            "real error= 157 \tobserved error on samples= 0 \toptimisation loss= 0.057061624\n",
            "real error= 155 \tobserved error on samples= 0 \toptimisation loss= 0.055768315\n",
            "real error= 157 \tobserved error on samples= 0 \toptimisation loss= 0.054512538\n",
            "real error= 154 \tobserved error on samples= 0 \toptimisation loss= 0.05334814\n",
            "real error= 169 \tobserved error on samples= 0 \toptimisation loss= 0.052216474\n",
            "real error= 156 \tobserved error on samples= 0 \toptimisation loss= 0.051080085\n",
            "real error= 156 \tobserved error on samples= 0 \toptimisation loss= 0.049926642\n",
            "real error= 159 \tobserved error on samples= 0 \toptimisation loss= 0.048869565\n",
            "real error= 156 \tobserved error on samples= 0 \toptimisation loss= 0.047851678\n",
            "real error= 154 \tobserved error on samples= 0 \toptimisation loss= 0.046874996\n",
            "real error= 173 \tobserved error on samples= 0 \toptimisation loss= 0.04598047\n",
            "real error= 163 \tobserved error on samples= 0 \toptimisation loss= 0.04504813\n",
            "real error= 165 \tobserved error on samples= 0 \toptimisation loss= 0.044063427\n",
            "real error= 167 \tobserved error on samples= 0 \toptimisation loss= 0.043160453\n",
            "real error= 156 \tobserved error on samples= 0 \toptimisation loss= 0.042288933\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}