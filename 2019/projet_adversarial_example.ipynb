{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMlAltYQeQip2oA89MR7FMI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fb8a202aff45473a933962b8627802e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_805be98959314f4681d54e1b7a4bd96f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_081cab8b63664cd7b5fe3f8e3ae3c87d",
              "IPY_MODEL_744493e83fc644ba926aa3b8357f248e"
            ]
          }
        },
        "805be98959314f4681d54e1b7a4bd96f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "081cab8b63664cd7b5fe3f8e3ae3c87d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a118ccbb71394560a548eac749e1f5f9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fc8d4c47c940488b88976fc6862aea27"
          }
        },
        "744493e83fc644ba926aa3b8357f248e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4d030ad7b88f4bd1b438124636a28139",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 528M/528M [00:07&lt;00:00, 78.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_49cc66cf5c5e4bad8f4f4dd73fe47179"
          }
        },
        "a118ccbb71394560a548eac749e1f5f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fc8d4c47c940488b88976fc6862aea27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4d030ad7b88f4bd1b438124636a28139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "49cc66cf5c5e4bad8f4f4dd73fe47179": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achanhon/coursdeeplearningcolab/blob/master/projet_adversarial_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpj8iTA1-3DD",
        "colab_type": "text"
      },
      "source": [
        "#Projet \"apprentissage par ordinateur et exemples adversaires\"\n",
        "\n",
        "Le projet porte sur un problème d'apprentissage \"simple\" (classification binaire), mais, à travers le prisme des exemples adversaires.\n",
        "\n",
        "Précisément, le projet s'effectue sur les données http://www.nlpr.ia.ac.cn/pal/trafficdata/recognition.html\n",
        "\n",
        "- Ce jeu de données permet de faire de la classification multiclasses, mais, il sera rendu binaire pour plus de simplicité: les 16 premières classes seront regroupés (limitation de vitesse et interdiction de tourner) dans la classe 1, tous les autres paneaux sont regroupés dans la classe 2.\n",
        "\n",
        "- De plus, afin de faciliter le traitement des images, toutes les images seront ramener à des images 96x96.\n",
        "\n",
        "**Mais** l'objectif n'est pas seulement d'apprendre un classifier (binaire) sur ces images 96x96. L'objectif est \n",
        "\n",
        "- de générer des images adversaires pour tromper un tel classifier\n",
        "\n",
        "**ou**\n",
        "\n",
        "- d'apprendre un classifier à la fois \"robuste\" et performant\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GODim6LhByA8",
        "colab_type": "text"
      },
      "source": [
        "##prise en main des données\n",
        "\n",
        "Télécharger les données. Le code ci dessous (**CHEMINS VERS LES DONNEES À MODIFIER**) permet de charger les données i.e. transformer cet ensemble d'image en une grosse matrice (4 matrices préciséments Xtrain,Xtest,Ytrain,Ytest)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONZnOvNRCLko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import PIL\n",
        "import PIL.Image\n",
        "\n",
        "print(\"LOADING DATA\")\n",
        "print(\"rescaling all images at 96x96\")\n",
        "print(\"merging classes in 2 super classes (speed limit and turn interdiction vs all)\")\n",
        "Xtrain = np.zeros((4170,3,96,96),dtype=int)\n",
        "Ytrain = np.zeros(4170,dtype=int)\n",
        "Xtest = np.zeros((1994,3,96,96),dtype=int)\n",
        "Ytest = np.zeros(1994,dtype=int)\n",
        "\n",
        "\n",
        "with open(\"build/TsignRecgTrain4170Annotation.txt\") as f:\n",
        "    reader = csv.reader(f,delimiter=\";\")\n",
        "    \n",
        "    for i,row in enumerate(reader):\n",
        "        image3D = np.asarray(PIL.Image.open(\"build/tsrd-train/\"+row[0]).convert(\"RGB\").copy().resize((96,96)),dtype=float)\n",
        "        Xtrain[i] = np.transpose(image3D,(2,0,1))\n",
        "        if int(row[7])<16:\n",
        "            Ytrain[i] = 0\n",
        "        else:\n",
        "            Ytrain[i] = 1\n",
        "        \n",
        "with open(\"build/TsignRecgTest1994Annotation.txt\") as f:\n",
        "    reader = csv.reader(f,delimiter=\";\")\n",
        "    \n",
        "    for i,row in enumerate(reader):\n",
        "        image3D = np.asarray(PIL.Image.open(\"build/TSRD-Test/\"+row[0]).convert(\"RGB\").copy().resize((96,96)),dtype=float)\n",
        "        Xtest[i] = np.transpose(image3D,(2,0,1))\n",
        "        if int(row[7])<16:\n",
        "            Ytest[i] = 0\n",
        "        else:\n",
        "            Ytest[i] = 1\n",
        "        \n",
        "print(\"nb of samples with class 0/1 in train=\",4170-np.sum(Ytrain),\"/\",np.sum(Ytrain))\n",
        "print(\"nb of samples with class 0/1 in test=\",4170-np.sum(Ytest),\"/\",np.sum(Ytest))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hbuPidyDPRM",
        "colab_type": "text"
      },
      "source": [
        "##référence sur les adversariaux\n",
        "\n",
        "Indépendamment, voilà quelques liens vers des articles de référence sur les adversariaux\n",
        "\n",
        "* Une introduction au problème des adversariaux : www.tensorflow.org/tutorials/generative/adversarial_fgsm\n",
        "* L'article correspondant arxiv.org/abs/1412.6572\n",
        "\n",
        "Du coté de la méthode de référence de \"protection\" contre les adversariaux\n",
        "* apprendre sur des données adversaires limites leur effet arxiv.org/abs/1710.10571\n",
        "* apprendre sur la pire données adversaires possibles permet d'avoir une robustesse certaines arxiv.org/abs/1711.00851\n",
        "\n",
        "**Important** : générer des données adversaires est très simple quand on a accès au gradient vis à vis de l'image elle même (typiquement si le modèle est du deep learning). Mais c'est possible aussi en approximant le gradient d'un modèle non deep : arxiv.org/pdf/1602.02697.pdf !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rhxa83zsAcv",
        "colab_type": "text"
      },
      "source": [
        "##La dérivée par rapport aux données\n",
        "\n",
        "Nous avons vu en cours que la façon même d'apprendre les poids des réseaux de neurones passe \n",
        "- par le calcul des dérivées par rapport aux valeurs des neurones\n",
        "- sachant que la dérivée par rapport à un neurones se calcule via la dérivée par rapport aux neurones suivants\n",
        "\n",
        "Cela s'applique aux valeurs des entrées !\n",
        "\n",
        "Informatiquement, il suffit de préciser qu'on veut stocker ces dérivées pour qu'elle soit calculées en utilisant *loss.backward()*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbn9PUYWtPNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.autograd\n",
        "import torch.autograd.variable\n",
        "\n",
        "mytarget = torch.randn(1,1,4,4)\n",
        "myinput = torch.autograd.Variable(torch.randn(1,3,4,4),requires_grad=True)\n",
        "mymodel = nn.Conv2d(3,1,kernel_size=3, padding=1)\n",
        "\n",
        "myloss = torch.sum((mytarget-mymodel(myinput))*(mytarget-mymodel(myinput)))\n",
        "\n",
        "print(\"classical learning aims to optimize weights of mymodel to decrease loss\")\n",
        "print(\"but here, one could optimize value of myinput to increase the loss eventually producing an adversarial example\")\n",
        "\n",
        "print(myloss)\n",
        "myloss.backward()\n",
        "\n",
        "print(\"here is the gradient of myloss regarding myinput\",myinput.grad.data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgafFG6aPofx",
        "colab_type": "text"
      },
      "source": [
        "##Une baseline\n",
        "\n",
        "Comme baseline pour commencer, il est possible (mais pas obligatoire) de considérer l'encodeur d'un VGG16 (2 convolutions - 1 pooling - 2 convolutions - 1 pooling - 3 convolutions - 1 pooling - 3 convolution - 1 pooling - 3 convolution - 1 pooling).\n",
        "\n",
        "Appliqué à une image 3x96x96, la sortie sera une image 512x3x3 à laquel on appliquera un dernier pooling 3x3 stride 1 sans padding pour obtenir un vecteur de dimension 512, il faudra ensuite ajouter un dernier neurone pour passer de 512 à 2 valeurs (pour apprendre avec une cross entropy - 1 valeur si vous voulez apprendre en en hinge loss).\n",
        "\n",
        "**le modèle VGG est directement téléchargeable depuis des dépôts pytorch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFhkqqwBRGxu",
        "colab_type": "code",
        "outputId": "a8624bb4-c8a7-42a6-fcca-669af7b04a6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "fb8a202aff45473a933962b8627802e0",
            "805be98959314f4681d54e1b7a4bd96f",
            "081cab8b63664cd7b5fe3f8e3ae3c87d",
            "744493e83fc644ba926aa3b8357f248e",
            "a118ccbb71394560a548eac749e1f5f9",
            "fc8d4c47c940488b88976fc6862aea27",
            "4d030ad7b88f4bd1b438124636a28139",
            "49cc66cf5c5e4bad8f4f4dd73fe47179"
          ]
        }
      },
      "source": [
        "import torchvision\n",
        "\n",
        "monvgg16 = torchvision.models.vgg16(pretrained=True, progress=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb8a202aff45473a933962b8627802e0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=553433881), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2IiHCcTRlhV",
        "colab_type": "code",
        "outputId": "be1ef791-4464-4e20-82f8-682d3f0e3c7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tensortest = torch.randn(1,3,96,96)\n",
        "print(monvgg16.features(tensortest).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 512, 3, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7bTAv-9Tsuu",
        "colab_type": "text"
      },
      "source": [
        "On a bien l'encodeur qu'on voulait"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pB66tDmSXjd",
        "colab_type": "code",
        "outputId": "91b3bd11-f2fc-44ca-ce88-8090b5e5fea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "monvgg16.avgpool = nn.AvgPool2d(kernel_size=(3, 3))\n",
        "monvgg16.classifier = None\n",
        "monvgg16.classifier = nn.Linear(512,2)\n",
        "print(monvgg16(tensortest).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efx7ymgbTvVN",
        "colab_type": "text"
      },
      "source": [
        "en remplaçant le classifier, on a obtient un modèle adapté à classer des images 96x96 !\n",
        "\n",
        "Il ne reste plus qu'à l'entrainer sur cette base..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGSf8XnpLDtr",
        "colab_type": "text"
      },
      "source": [
        "##Maintenant à vous de jouer !"
      ]
    }
  ]
}