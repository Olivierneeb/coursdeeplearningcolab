{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achanhon/coursdeeplearningcolab/blob/master/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oevBPEjs-ebR",
        "colab_type": "text"
      },
      "source": [
        "# TP optimisation d'un réseau de neurones\n",
        "\n",
        "L'objectif de ce TP est de manipuler les notions liées à l'optimisation de réseau de neurones.\n",
        "\n",
        "L'idée est de partir d'un code qui fonctionne avec torch (torch faisant toute l'optimisation) et d'essayer de supprimer un maximum de fonction de torch.\n",
        "\n",
        "**CEPENDANT, si vous n'avez pas fini le précédent TP, il est préférable de le terminer (convolution / pooling) plutôt que de commencer celui ci**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkYSIs-uh_ys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.autograd\n",
        "import torch.autograd.variable\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def visualize_current_model_behaviour(X,Y,model):\n",
        "    grid = np.ones((50,50,3),dtype=int)*255\n",
        "    \n",
        "    batch = np.zeros((50*50,2),dtype=float)\n",
        "    for row in range(50):\n",
        "        for col in range(50):\n",
        "            batch[row*50+col][0]=row\n",
        "            batch[row*50+col][1]=col\n",
        "    prediction = model.getPredictedClass(batch)\n",
        "    \n",
        "    pred = np.ones((50,50,3),dtype=int)*255\n",
        "    for row in range(50):\n",
        "        for col in range(50):\n",
        "            if prediction[row*50+col] == 1:\n",
        "                grid[row][col][0]=175\n",
        "                grid[row][col][2]=175\n",
        "            else:\n",
        "                grid[row][col][0]=175\n",
        "                grid[row][col][1]=175\n",
        "    \n",
        "    for i in range(X.shape[0]):\n",
        "        row,col = X[i][0],X[i][1]\n",
        "        if Y[i]==1:\n",
        "            grid[row][col][0]=0\n",
        "            grid[row][col][2]=0\n",
        "        else:\n",
        "            grid[row][col][0]=0\n",
        "            grid[row][col][1]=0\n",
        "    \n",
        "    return grid\n",
        "\n",
        "\n",
        "def train_test_deep_network(X,Y,model,nbIteration):\n",
        "    model.updateweights((X,Y))\n",
        "\n",
        "    for iteration in range(nbIteration-1):\n",
        "        loss = model.updateweights((X,Y))\n",
        "        \n",
        "        if iteration%50==0:\n",
        "          visu = visualize_current_model_behaviour(X,Y,model)\n",
        "\n",
        "          clear_output()\n",
        "          plt.imshow(visu)\n",
        "          plt.show()\n",
        "          sleep(3)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def getPredictedClass(self,x):\n",
        "        variablex = torch.autograd.Variable(torch.Tensor(x.astype(float)))\n",
        "        variableoutput = self.forward(variablex)\n",
        "        prob = variableoutput.cpu().data.numpy()\n",
        "        return np.argmax(prob,axis=1)\n",
        "    \n",
        "    def updateweights(self,batchfromtrain):\n",
        "        x,y = batchfromtrain\n",
        "        variablex = torch.autograd.Variable(torch.Tensor(x.astype(float)))\n",
        "        variabley = torch.autograd.Variable(torch.from_numpy(y).long())\n",
        "        variableoutput = self.forward(variablex)\n",
        "        \n",
        "        loss = self.losslayer(variableoutput,variabley)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()        \n",
        "        return loss.cpu().data.numpy()\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 30, bias=True)\n",
        "        self.fc2 = nn.Linear(30, 30, bias=True)\n",
        "        self.fc2bis = nn.Linear(30, 30, bias=True)\n",
        "        self.fc3 = nn.Linear(30, 2, bias=True)\n",
        "        \n",
        "        self.train()\n",
        "        \n",
        "        \n",
        "        self.lr = 0.1\n",
        "        self.momentum = 0.5\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=self.lr, momentum=self.momentum)\n",
        "        self.losslayer = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x/30))\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = F.leaky_relu(self.fc2bis(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNw7ge7GBWuM",
        "colab_type": "text"
      },
      "source": [
        "ci dessus un exemple de code d'apprentissage : tout est fait dans la fonction model.updateweights((X,Y))\n",
        "\n",
        "notamment, dans les 3 lignes de code \n",
        "self.optimizer.zero_grad()\n",
        "loss.backward()\n",
        "self.optimizer.step()    \n",
        "\n",
        "ci dessous, un exemple de comment on s'en sert sur des données\n",
        "\n",
        "Penser aussi à jeter un coup d'oeil à https://playground.tensorflow.org qui fait globalement la même chose que le code ci dessus mais en beaucoup mieux et en beaucoup plus beau\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjba31Gc_5VL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net()\n",
        "\n",
        "Y = np.array([1,1,1,1,1,1,1,1,-1,1,1,-1,-1,1,-1,-1,-1,-1,-1,-1,-1,-1,1,1,-1,1,-1,1,1,1,1])\n",
        "Y = np.maximum(Y,np.zeros(Y.shape[0]))\n",
        "X = np.zeros((Y.shape[0],2),dtype=int)\n",
        "X[ 1 ]=np.array([ 2 , 22 ])\n",
        "X[ 2 ]=np.array([ 2 , 37 ])\n",
        "X[ 3 ]=np.array([ 7 , 17 ])\n",
        "X[ 4 ]=np.array([ 7 , 22 ])\n",
        "X[ 5 ]=np.array([ 7 , 32 ])\n",
        "X[ 6 ]=np.array([ 12 , 7 ])\n",
        "X[ 7 ]=np.array([ 12 , 12 ])\n",
        "X[ 8 ]=np.array([ 12 , 22 ])\n",
        "X[ 9 ]=np.array([ 12 , 37 ])\n",
        "X[ 10 ]=np.array([ 17 , 7 ])\n",
        "X[ 11 ]=np.array([ 17 , 22 ])\n",
        "X[ 12 ]=np.array([ 17 , 27 ])\n",
        "X[ 13 ]=np.array([ 17 , 37 ])\n",
        "X[ 14 ]=np.array([ 22 , 17 ])\n",
        "X[ 15 ]=np.array([ 27 , 17 ])\n",
        "X[ 16 ]=np.array([ 27 , 22 ])\n",
        "X[ 17 ]=np.array([ 27 , 27 ])\n",
        "X[ 18 ]=np.array([ 27 , 37 ])\n",
        "X[ 19 ]=np.array([ 32 , 22 ])\n",
        "X[ 20 ]=np.array([ 32 , 32 ])\n",
        "X[ 21 ]=np.array([ 32 , 37 ])\n",
        "X[ 22 ]=np.array([ 37 , 12 ])\n",
        "X[ 23 ]=np.array([ 37 , 17 ])\n",
        "X[ 24 ]=np.array([ 37 , 22 ])\n",
        "X[ 25 ]=np.array([ 37 , 27 ])\n",
        "X[ 26 ]=np.array([ 37 , 37 ])\n",
        "X[ 27 ]=np.array([ 42 , 22 ])\n",
        "X[ 28 ]=np.array([ 42 , 27 ])\n",
        "X[ 29 ]=np.array([ 42 , 32 ])\n",
        "X[ 30 ]=np.array([ 42 , 37 ])\n",
        "\n",
        "\n",
        "train_test_deep_network(X,Y,model,300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3GRCMQSEvTD",
        "colab_type": "text"
      },
      "source": [
        "**L'objectif de ce TP est de coder vous même : loss.backward() et self.optimizer.step()**\n",
        "\n",
        "Penser à vous aider de la documentation https://pytorch.org/ et de numpy !\n",
        "\n",
        "**IMPORTANT :**\n",
        "\n",
        "les couches sont donc désormais des objets numpy !\n",
        "\n",
        "la loss ne peut plus être une cross entropy... à la place, utiliser la hingeloss : loss = model.relu(1-y*(proba_classe_1-proba_classe_0))\n",
        "\n",
        "ne chercher pas à visualiser en même temps que vous calculer (c'est déjà assez compliqué) - chercher juste à diminuer la loss\n",
        "\n",
        "**coder l'optimisation dans la cellule \"NET\" puis lancer (plusieurs fois d'affiler) la cellule d'après - la loss doit diminuer - chaque lancer correspond à 10 pas de gradient**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hl1fWvSGMiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.autograd\n",
        "import torch.autograd.variable\n",
        "\n",
        "import collections \n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def updateweights(self,batch):\n",
        "        allX,allY = batch\n",
        "        allX,allY = shuffle(allX,allY)\n",
        "        totalloss = 0\n",
        "        \n",
        "        for sample in range(len(allX)):\n",
        "            x = allX[sample].copy()\n",
        "            y = allY[sample].copy()\n",
        "        \n",
        "            print(\"TODO\")\n",
        "    \n",
        "        return totalloss\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = np.random.randn(30,2)\n",
        "        self.fc1bias = np.random.randn(30)\n",
        "        \n",
        "        self.fc2 = np.random.randn(30,30)\n",
        "        self.fc2bias = np.random.randn(30)\n",
        "        \n",
        "        self.fc2bis = np.random.randn(30,30)\n",
        "        self.fc2biasbis = np.random.randn(30)\n",
        "        \n",
        "        self.fc3 = np.random.randn(2,30)\n",
        "        self.fc3bias = np.random.randn(2)\n",
        "        \n",
        "        self.w = [self.fc1,self.fc2,self.fc2bis,self.fc3]\n",
        "        self.b = [self.fc1bias,self.fc2bias,self.fc2biasbis,self.fc3bias]\n",
        "\n",
        "    def leaky_relu(self,v):\n",
        "        if v>0:\n",
        "            return v\n",
        "        else:\n",
        "            return v/100\n",
        "    def relu(self,v):\n",
        "        if v>0:\n",
        "            return v\n",
        "        else:\n",
        "            return 0\n",
        "        \n",
        "model = Net()\n",
        "\n",
        "Y = np.array([1,1,1,1,1,1,1,1,-1,1,1,-1,-1,1,-1,-1,-1,-1,-1,-1,-1,-1,1,1,-1,1,-1,1,1,1,1])\n",
        "Y = np.maximum(Y,np.zeros(Y.shape[0]))\n",
        "X = np.zeros((Y.shape[0],2),dtype=int)\n",
        "X[ 1 ]=np.array([ 2 , 22 ])\n",
        "X[ 2 ]=np.array([ 2 , 37 ])\n",
        "X[ 3 ]=np.array([ 7 , 17 ])\n",
        "X[ 4 ]=np.array([ 7 , 22 ])\n",
        "X[ 5 ]=np.array([ 7 , 32 ])\n",
        "X[ 6 ]=np.array([ 12 , 7 ])\n",
        "X[ 7 ]=np.array([ 12 , 12 ])\n",
        "X[ 8 ]=np.array([ 12 , 22 ])\n",
        "X[ 9 ]=np.array([ 12 , 37 ])\n",
        "X[ 10 ]=np.array([ 17 , 7 ])\n",
        "X[ 11 ]=np.array([ 17 , 22 ])\n",
        "X[ 12 ]=np.array([ 17 , 27 ])\n",
        "X[ 13 ]=np.array([ 17 , 37 ])\n",
        "X[ 14 ]=np.array([ 22 , 17 ])\n",
        "X[ 15 ]=np.array([ 27 , 17 ])\n",
        "X[ 16 ]=np.array([ 27 , 22 ])\n",
        "X[ 17 ]=np.array([ 27 , 27 ])\n",
        "X[ 18 ]=np.array([ 27 , 37 ])\n",
        "X[ 19 ]=np.array([ 32 , 22 ])\n",
        "X[ 20 ]=np.array([ 32 , 32 ])\n",
        "X[ 21 ]=np.array([ 32 , 37 ])\n",
        "X[ 22 ]=np.array([ 37 , 12 ])\n",
        "X[ 23 ]=np.array([ 37 , 17 ])\n",
        "X[ 24 ]=np.array([ 37 , 22 ])\n",
        "X[ 25 ]=np.array([ 37 , 27 ])\n",
        "X[ 26 ]=np.array([ 37 , 37 ])\n",
        "X[ 27 ]=np.array([ 42 , 22 ])\n",
        "X[ 28 ]=np.array([ 42 , 27 ])\n",
        "X[ 29 ]=np.array([ 42 , 32 ])\n",
        "X[ 30 ]=np.array([ 42 , 37 ])\n",
        "\n",
        "memoryofloss = collections.deque(maxlen=200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgcbPga6afnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for iteration in range(10):\n",
        "    loss = model.updateweights((X,Y))\n",
        "    memoryofloss.append(loss)\n",
        "print(sum(memoryofloss)/len(memoryofloss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZUMmTmLGPAj",
        "colab_type": "text"
      },
      "source": [
        "**question bonus/alternative**\n",
        "\n",
        "Le méchanisme de rétro propagation du gradient permet de calculer le gradient vis à vis de l'entrée : cela permet de calculer comment il faudrait modifier un point pour qu'il soit mal classé !\n",
        "\n",
        "Taper *adversarial attack* dans google pour quelles illustrations.\n",
        "\n",
        "Faites un descente de gradient (en utilisant les objets torch) sur un point de la base pour l'amener hors de la zone correcte.\n",
        "\n",
        "\n",
        "*Cette question est conceptuellement plus compliqué que la précédente mais finalement moins compliquée en pratique*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1UeS0ZqH8ED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"TODO\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}