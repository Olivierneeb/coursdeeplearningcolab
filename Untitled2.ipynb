{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achanhon/coursdeeplearningcolab/blob/master/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oevBPEjs-ebR",
        "colab_type": "text"
      },
      "source": [
        "# TP deep learning\n",
        "\n",
        "L'objectif de ce TP est de manipuler les notions liées à l'optimisation de réseau de neurones.\n",
        "\n",
        "L'idée est de partir d'un code qui fonctionne avec torch (torch faisant toute l'optimisation) et d'essayer de supprimer un maximum de fonction de torch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkYSIs-uh_ys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.autograd\n",
        "import torch.autograd.variable\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def visualize_current_model_behaviour(X,Y,model):\n",
        "    grid = np.ones((50,50,3),dtype=int)*255\n",
        "    \n",
        "    batch = np.zeros((50*50,2),dtype=float)\n",
        "    for row in range(50):\n",
        "        for col in range(50):\n",
        "            batch[row*50+col][0]=row\n",
        "            batch[row*50+col][1]=col\n",
        "    prediction = model.getPredictedClass(batch)\n",
        "    \n",
        "    pred = np.ones((50,50,3),dtype=int)*255\n",
        "    for row in range(50):\n",
        "        for col in range(50):\n",
        "            if prediction[row*50+col] == 1:\n",
        "                grid[row][col][0]=175\n",
        "                grid[row][col][2]=175\n",
        "            else:\n",
        "                grid[row][col][0]=175\n",
        "                grid[row][col][1]=175\n",
        "    \n",
        "    for i in range(X.shape[0]):\n",
        "        row,col = X[i][0],X[i][1]\n",
        "        if Y[i]==1:\n",
        "            grid[row][col][0]=0\n",
        "            grid[row][col][2]=0\n",
        "        else:\n",
        "            grid[row][col][0]=0\n",
        "            grid[row][col][1]=0\n",
        "    \n",
        "    return grid\n",
        "\n",
        "\n",
        "def train_test_deep_network(X,Y,model,nbIteration):\n",
        "    model.updateweights((X,Y))\n",
        "\n",
        "    for iteration in range(nbIteration-1):\n",
        "        loss = model.updateweights((X,Y))\n",
        "        \n",
        "        if iteration%50==0:\n",
        "          visu = visualize_current_model_behaviour(X,Y,model)\n",
        "\n",
        "          clear_output()\n",
        "          plt.imshow(visu)\n",
        "          plt.show()\n",
        "          sleep(3)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def getPredictedClass(self,x):\n",
        "        variablex = torch.autograd.Variable(torch.Tensor(x.astype(float)))\n",
        "        variableoutput = self.forward(variablex)\n",
        "        prob = variableoutput.cpu().data.numpy()\n",
        "        return np.argmax(prob,axis=1)\n",
        "    \n",
        "    def updateweights(self,batchfromtrain):\n",
        "        x,y = batchfromtrain\n",
        "        variablex = torch.autograd.Variable(torch.Tensor(x.astype(float)))\n",
        "        variabley = torch.autograd.Variable(torch.from_numpy(y).long())\n",
        "        variableoutput = self.forward(variablex)\n",
        "        \n",
        "        loss = self.losslayer(variableoutput,variabley)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()        \n",
        "        return loss.cpu().data.numpy()\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 30, bias=True)\n",
        "        self.fc2 = nn.Linear(30, 30, bias=True)\n",
        "        self.fc2bis = nn.Linear(30, 30, bias=True)\n",
        "        self.fc3 = nn.Linear(30, 2, bias=True)\n",
        "        \n",
        "        self.train()\n",
        "        \n",
        "        \n",
        "        self.lr = 0.1\n",
        "        self.momentum = 0.5\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=self.lr, momentum=self.momentum)\n",
        "        self.losslayer = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x/30))\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = F.leaky_relu(self.fc2bis(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNw7ge7GBWuM",
        "colab_type": "text"
      },
      "source": [
        "ci dessus un exemple de code d'apprentissage : tout est fait dans la fonction model.updateweights((X,Y))\n",
        "\n",
        "notamment, dans les 3 lignes de code \n",
        "self.optimizer.zero_grad()\n",
        "loss.backward()\n",
        "self.optimizer.step()    \n",
        "\n",
        "ci dessous, un exemple de comment on s'en sert sur des données\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjba31Gc_5VL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "9c25990e-02d2-4006-ab9e-b1a24e5e16f0"
      },
      "source": [
        "model = Net()\n",
        "\n",
        "Y = np.array([1,1,1,1,1,1,1,1,-1,1,1,-1,-1,1,-1,-1,-1,-1,-1,-1,-1,-1,1,1,-1,1,-1,1,1,1,1])\n",
        "Y = np.maximum(Y,np.zeros(Y.shape[0]))\n",
        "X = np.zeros((Y.shape[0],2),dtype=int)\n",
        "X[ 1 ]=np.array([ 2 , 22 ])\n",
        "X[ 2 ]=np.array([ 2 , 37 ])\n",
        "X[ 3 ]=np.array([ 7 , 17 ])\n",
        "X[ 4 ]=np.array([ 7 , 22 ])\n",
        "X[ 5 ]=np.array([ 7 , 32 ])\n",
        "X[ 6 ]=np.array([ 12 , 7 ])\n",
        "X[ 7 ]=np.array([ 12 , 12 ])\n",
        "X[ 8 ]=np.array([ 12 , 22 ])\n",
        "X[ 9 ]=np.array([ 12 , 37 ])\n",
        "X[ 10 ]=np.array([ 17 , 7 ])\n",
        "X[ 11 ]=np.array([ 17 , 22 ])\n",
        "X[ 12 ]=np.array([ 17 , 27 ])\n",
        "X[ 13 ]=np.array([ 17 , 37 ])\n",
        "X[ 14 ]=np.array([ 22 , 17 ])\n",
        "X[ 15 ]=np.array([ 27 , 17 ])\n",
        "X[ 16 ]=np.array([ 27 , 22 ])\n",
        "X[ 17 ]=np.array([ 27 , 27 ])\n",
        "X[ 18 ]=np.array([ 27 , 37 ])\n",
        "X[ 19 ]=np.array([ 32 , 22 ])\n",
        "X[ 20 ]=np.array([ 32 , 32 ])\n",
        "X[ 21 ]=np.array([ 32 , 37 ])\n",
        "X[ 22 ]=np.array([ 37 , 12 ])\n",
        "X[ 23 ]=np.array([ 37 , 17 ])\n",
        "X[ 24 ]=np.array([ 37 , 22 ])\n",
        "X[ 25 ]=np.array([ 37 , 27 ])\n",
        "X[ 26 ]=np.array([ 37 , 37 ])\n",
        "X[ 27 ]=np.array([ 42 , 22 ])\n",
        "X[ 28 ]=np.array([ 42 , 27 ])\n",
        "X[ 29 ]=np.array([ 42 , 32 ])\n",
        "X[ 30 ]=np.array([ 42 , 37 ])\n",
        "\n",
        "\n",
        "train_test_deep_network(X,Y,model,300)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD6CAYAAABnLjEDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3GRCMQSEvTD",
        "colab_type": "text"
      },
      "source": [
        "**L'objectif de ce TP est de coder vous même : loss.backward() et self.optimizer.step()**\n",
        "\n",
        "Penser à vous aider de la documentation https://pytorch.org/\n",
        "\n",
        "Penser aussi à jeter un coup d'oeil à https://playground.tensorflow.org qui fait globalement la même chose que le code ci dessus mais en beaucoup mieux et en beaucoup plus beau\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hl1fWvSGMiN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f4c6a0c-c668-45ac-dcb1-610281fa3c4a"
      },
      "source": [
        "print(\"TODO\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TODO\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZUMmTmLGPAj",
        "colab_type": "text"
      },
      "source": [
        "**question alternative**\n",
        "\n",
        "Le méchanisme de rétro propagation du gradient permet de calculer le gradient vis à vis de l'entrée : cela permet de calculer comment il faudrait modifier un point pour qu'il soit mal classé !\n",
        "\n",
        "Taper *adversarial attack* dans google pour quelles illustrations.\n",
        "\n",
        "Faites un descente de gradient (en utilisant les objets torch) sur un point de la base pour l'amener hors de la zone correcte.\n",
        "\n",
        "\n",
        "*Cette question est conceptuellement plus compliqué que la précédente mais finalement moins compliquée en pratique*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1UeS0ZqH8ED",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7e4ff1b3-d6fc-44ab-80da-ed0ccb5dbac9"
      },
      "source": [
        "print(\"TODO\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TODO\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}