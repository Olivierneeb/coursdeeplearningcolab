{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMmEDq3RY871aV/SX5ZQPfg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achanhon/coursdeeplearningcolab/blob/master/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWdcxZdBXNp7",
        "colab_type": "text"
      },
      "source": [
        "#Formation deep learning\n",
        "\n",
        "L'objet de ce TP est de présenter brievement les outils de deep learning Pytorch. Il comporte 3 parties\n",
        "\n",
        "1.   Les briques de bases\n",
        "2.   Le traitement d'ECG par convolution\n",
        "3.   Les réseaux récurrents\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVtpJwPzbtSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.autograd\n",
        "import torch.autograd.variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWiZxWIaZvjd",
        "colab_type": "text"
      },
      "source": [
        "##Les briques de bases\n",
        "\n",
        "La brique de base de pytorch est l'objet Variable : il stocke en interne les opérations effectués sur un tenseur, de sorte à pouvoir calculer **automatiquement** le gradient d'une valeur par rapport aux autres.\n",
        "Considérons le problème de moindre carré où on veut minimiser beta*beta + (Y-X*beta)^2 : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHyz0wNpXNE_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "ae598061-afc3-493a-ff55-e92fc8ab23e0"
      },
      "source": [
        "Y,X,beta = torch.rand(11),torch.rand((11,3)),torch.autograd.Variable(torch.rand(3),requires_grad=True)\n",
        "loss = torch.sum((Y-torch.mv(X,beta))*(Y-torch.mv(X,beta)))+torch.sum(beta*beta)\n",
        "print(X,Y,beta)\n",
        "print(\"initial loss=\",loss)\n",
        "loss.backward()\n",
        "print(beta.grad)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2867, 0.8855, 0.0706],\n",
            "        [0.1564, 0.5548, 0.8191],\n",
            "        [0.8308, 0.4533, 0.4845],\n",
            "        [0.7733, 0.3645, 0.3446],\n",
            "        [0.5724, 0.1961, 0.0520],\n",
            "        [0.4039, 0.3629, 0.1146],\n",
            "        [0.1408, 0.5012, 0.6988],\n",
            "        [0.7207, 0.4853, 0.6481],\n",
            "        [0.3832, 0.8808, 0.5909],\n",
            "        [0.6938, 0.2765, 0.4949],\n",
            "        [0.5502, 0.5283, 0.7602]]) tensor([0.8144, 0.3587, 0.8440, 0.7820, 0.4974, 0.7291, 0.2919, 0.9737, 0.3254,\n",
            "        0.9734, 0.3489]) tensor([0.0734, 0.5652, 0.1398], requires_grad=True) tensor(2.1004, grad_fn=<AddBackward0>)\n",
            "tensor([-3.6712, -0.7445, -1.3325])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvIVYOkTeDj7",
        "colab_type": "text"
      },
      "source": [
        "Ci dessus le gradient est automatiquement calculé, permettant par exemple de faire une descente de gradient à la main simplement. Cela dit, des optimiseurs (typiquement la descente de gradient) sont déjà précodés : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoTpV9ZkeWFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.SGD([beta], lr=0.01, momentum=0.5)\n",
        "for i in range(10):\n",
        "  loss = torch.sum((Y-torch.mv(X,beta))*(Y-torch.mv(X,beta)))+torch.sum(beta*beta)\n",
        "  print(loss)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "print(\"final loss=\", print(loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6TA4kTcyGuZ",
        "colab_type": "code",
        "outputId": "ad5910d8-6c91-4e11-e653-a457e76cf298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "test = pd.read_csv(\"https://raw.githubusercontent.com/achanhon/coursdeeplearningcolab/master/ECG_data/rahh.csv\")\n",
        "print(test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   a   b   c\n",
            "0  0   1   2\n",
            "1  1   2   3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwlwHBha3I2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget \"https://github.com/achanhon/coursdeeplearningcolab/blob/master/ECG_data/X.pkl?raw=true\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}